# Appendix: Answers to Chapter Exercises {-}

This appendix provides solutions to the exercises given at the end of each chapter. These solutions are intended to help you verify your work and understand the correct approach to each task.

## Answers to Chapter 1 Practice Exercises {.unnumbered}

### Exercise 1: Familiarization with R Studio {.unnumbered}

1.  **Create a new R script and save it**
    -   Open R Studio, go to `File > New File > R Script`. This will open a new script tab in the Source Pane.
    -   Save the script by clicking `File > Save As...`, and name it `practice_script.R`.
2.  **Write and run a simple calculation**
    -   In the script, write the following line of code:

        ``` r
        8 * 9
        ```

    -   To run this line, place your cursor on the line and press `Ctrl + Enter` (Windows) or `Cmd + Enter` (macOS).
3.  **Comment your code**
    -   Add a comment above the code explaining what it does:

        ``` r
        # This code calculates the product of 8 and 9
        8 * 9
        ```

### Exercise 2: Basic Data Entry and Operation {.unnumbered}

1.  **Create a vector of numbers**
    -   Write the following line in an R script to create the vector:

        ``` r
        numbers <- 1:10
        ```
2.  **Calculate the sum of the vector**
    -   To calculate and print the sum, add this line to your script:

        ``` r
        print(sum(numbers))
        ```
3.  **Save the script**
    -   Ensure your work is saved in the script `practice_script.R` or in a new script file if preferred.

### Exercise 3: Introduction to R Markdown {.unnumbered}

1.  **Create a new R Markdown document**
    -   Go to `File > New File > R Markdown...`, provide a title "My First R Markdown", and fill in your name as the author.
2.  **Write a brief introduction**
    -   In the document, use the following Markdown syntax:

        ``` markdown
        # About Me
        This is a brief introduction about myself.
        - I am learning R and R Studio.
        - I enjoy data analysis.
        **Bold Fact**: I aim to be a data scientist.
        ```
3.  **Embed a chunk of R code**
    -   Include a code chunk that calculates the square of 12:

```{r}
12^2
```

4.  **Knit the document to HTML**
    -   Click the `Knit` button and select `Knit to HTML`. Save the output in your project directory.

### Exercise 4: Exploring the Help Pane {.unnumbered}

1.  **Find help on the `plot` function**
    -   In the Console, type `?plot` and press Enter. Review the help file that appears in the Help pane.
2.  **Write a command to plot a graph**
    -   In an R script, add the following line to plot a graph:

        ``` r
        plot(1:10, 1:10)
        ```
3.  **Add a title to the plot**
    -   Modify the plot command to include a title:

        ``` r
        plot(1:10, 1:10, main = "Simple Linear Plot")
        ```

## Answers to Chapter 2 Practice Exercises {.unnumbered}

### Exercise 1: Identifying Data Types {.unnumbered}

1.  **Scenario Analysis**:
    -   **Children at playground**: The data collection method used here is **observational data**. The psychologist is observing natural behaviors without intervening or manipulating the environment.
    -   **Evening diary entries**: This scenario uses **self-report data** as participants are providing personal accounts of their feelings and activities.
    -   **Noise level manipulation**: This is an example of **experimental manipulation**, where a variable (noise level) is deliberately changed to observe its effect on another variable (productivity).

### Exercise 2: Designing a Study {.unnumbered}

1.  **Study Design**:
    -   **Research Question**: Does listening to classical music while studying improve memory recall?
    -   **Type of Data**: Experimental manipulation.
    -   **Data Collection Method**: Participants are randomly assigned to two groups. One group studies in silence while the other listens to classical music. Afterwards, both groups take a memory test based on the material studied.
    -   **Ethical Considerations**: Ensure that participants are aware they can withdraw at any time and that all data collected will be confidential. Consider any potential stress or anxiety induced by test conditions and address these in the study design.

### Exercise 3: Evaluating Research {.unnumbered}

1.  **Research Evaluation**:
    -   **Type of Data Used**: Assuming the study involves assessing the effects of sleep on cognitive performance using different sleep interventions, the data type would likely be **experimental manipulation**.
    -   **Potential Biases**: If the study does not adequately randomize participants or control for other factors affecting sleep (like caffeine intake or room conditions), results could be biased.
    -   **Influence on Conclusions**: The use of experimental manipulation allows the researcher to make stronger causal claims about the effect of sleep on cognitive performance compared to observational or self-report data. However, biases and experimental design flaws can undermine these claims.

## Answers to Practice Exercises {.unnumbered}

### Exercise 1: Evaluating Reliability {.unnumbered}

1.  **Scenario Analysis**:
    -   **Answer**: The Pearson correlation coefficient of 0.65 indicates moderate test-retest reliability. While this isn't considered low, for measures of psychological constructs such as self-esteem, a higher coefficient (typically 0.7 or above) is generally preferred to ensure consistency over time. A coefficient of 0.65 might suggest that the questionnaire could benefit from further refinement to improve reliability.

### Exercise 2: Assessing Validity {.unnumbered}

1.  **Scenario Development**:
    -   **Answer**: Steps to validate the aptitude test could include:
        -   **Developing a Hypothesis**: Predict that high scores on the aptitude test correlate with higher academic performance in college.
        -   **Collecting Data**: Gather test scores from incoming college students and their subsequent grade point averages (GPAs) at the end of their first year.
        -   **Statistical Analysis**: Perform a correlation analysis to assess the relationship between test scores and GPAs.
        -   **Interpreting Results**: A strong positive correlation would indicate good predictive validity of the aptitude test for college success.

### Exercise 3: Identifying and Addressing Data Collection Errors {.unnumbered}

1.  **Problem Solving**:
    -   **Answer**: The miscalibration of the sleep quality device could lead to inaccurate data, potentially skewing the study results. To mitigate this impact:
        -   **Re-calibrate the device**: Immediately correct the calibration error for future data collection.
        -   **Analyze impacted data**: Assess the extent of the data affected by the miscalibration and consider excluding or adjusting this data in the analysis.
        -   **Transparency in Reporting**: Disclose the issue and the steps taken to address it in any publications or presentations involving this research.

### Exercise 4: Triangulation to Enhance Validity {.unnumbered}

1.  **Critical Thinking**:
    -   **Answer**: Using multiple data sources like surveys, observations, and performance metrics helps enhance the construct validity of the study. This triangulation approach allows for validation of the findings through different perspectives, reducing the bias that might be present if only one method were used. Each method complements the others, providing a more holistic view of student engagement.

### Exercise 5: Role Play on Ethical Data Collection {.unnumbered}

1.  **Discussion**:
    -   **Answer**: Key procedures and safeguards might include:
        -   **Informed Consent**: Ensure all participants are fully aware of the nature of the data being collected and its intended use. Obtain written consent.
        -   **Anonymity and Confidentiality**: Assign codes to participants instead of using names and store personal data securely. Ensure that any reports or publications do not allow individual participants to be identified.
        -   **Minimizing Harm**: Be sensitive to how questions about personal health might affect participants and provide support resources as necessary.

### Exercise 6: Real-World Application {.unnumbered}

1.  **Application**:
    -   **Answer**: This exercise is subjective and would depend on the specific study chosen. Generally, the answer should include an evaluation of the methods section for clarity on measurement tools, reliability coefficients, validity assertions, and a discussion on how well the study accounted for potential data collection errors. Suggestions for improvement might include more rigorous reliability testing, additional validation studies, or enhanced error checking procedures.

## Answers to Chapter 4 Practice Exercises {.unnumbered}

### Exercise 1: Calculating Descriptive Statistics {.unnumbered}

**Dataset**: `c(55, 65, 75, 85, 95, 105, 115, 125, 135, 145)`

```{r}
# Sample data vector
scores <- c(55, 65, 75, 85, 95, 105, 115, 125, 135, 145)

# Calculate mean
mean_score <- mean(scores)
print(paste("Mean:", mean_score))

# Calculate median
median_score <- median(scores)
print(paste("Median:", median_score))

# Calculate mode
get_mode <- function(x) {
  uniqv <- unique(x)
  uniqv[which.max(tabulate(match(x, uniqv)))]
}
mode_score <- get_mode(scores)
print(paste("Mode:", mode_score))

# Calculate variance
variance_value <- var(scores)
print(paste("Variance:", variance_value))

# Calculate standard deviation
std_deviation <- sd(scores)
print(paste("Standard Deviation:", std_deviation))

# Identify outliers using IQR
Q1 <- quantile(scores, 0.25)
Q3 <- quantile(scores, 0.75)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
outliers <- scores[scores < lower_bound | scores > upper_bound]

print(paste("Outliers:", paste(outliers, collapse = ", ")))

```

**Interpretation**:

-   **Mean**: 100

-   **Median**: 100

-   **Mode**: Since all values are unique, there is no mode in this dataset.

-   **Variance**: 1100

-   **Standard Deviation**: 33.16625

-   **Outliers**: There are no outliers in this dataset as all values lie within the lower and upper bounds.

### Exercise 2: Understanding the Normal Distribution {.unnumbered}

Assume a psychological test follows a normal distribution with a mean of 100 and a standard deviation of 15.

```{r}
# Parameters
mean <- 100
sd <- 15

# Probability of a score less than 85
prob_less_than_85 <- pnorm(85, mean, sd)
print(paste("Probability of a score less than 85:", prob_less_than_85))

# Probability of a score between 85 and 115
prob_between_85_and_115 <- pnorm(115, mean, sd) - pnorm(85, mean, sd)
print(paste("Probability of a score between 85 and 115:", prob_between_85_and_115))

```

**Interpretation**:

-   **Probability of a score less than 85**: 0.1586553 (or 15.87%)

-   **Probability of a score between 85 and 115**: 0.6826895 (or 68.27%)

### Exercise 3: Applying the T-Distribution {.unnumbered}

You are conducting a small-scale study with 12 participants.

```{r}
# Degrees of freedom
df <- 11  # for n = 12, df = n - 1

# Probability of a t-score less than 1.5
prob_less_than_1_5 <- pt(1.5, df)
print(paste("Probability of a t-score less than 1.5:", prob_less_than_1_5))

# Probability of a t-score between -1 and 1
prob_between_minus1_and_1 <- pt(1, df) - pt(-1, df)
print(paste("Probability of a t-score between -1 and 1:", prob_between_minus1_and_1))
```

**Interpretation**:

-   **Probability of a t-score less than 1.5**: 0.9180312 (or 91.80%)

-   **Probability of a t-score between -1 and 1**: 0.5764421 (or 57.64%)

### Exercise 4: Defining and Simulating Sample Spaces {.unnumbered}

Define a sample space for a study where participants can choose between three types of exercises (Yoga, Pilates, Aerobics). Simulate responses from 100 participants.

```{r}
# Define the sample space
sample_space <- c("Yoga", "Pilates", "Aerobics")

# Simulate responses from 100 participants
set.seed(123)  # For reproducibility
responses <- sample(sample_space, 100, replace = TRUE)

# Display the first 10 responses
print(responses[1:10])

# Analyze the frequency of each exercise choice
exercise_frequency <- table(responses)
print(exercise_frequency)
```

**Interpretation**:

-   **Sample Space**: {Yoga, Pilates, Aerobics}

-   **Simulated Responses (first 10)**: ["Pilates", "Yoga", "Yoga", "Yoga", "Aerobics", "Yoga", "Yoga", "Yoga", "Pilates", "Yoga"]

-   **Frequency Analysis**:

    -   Yoga: 34

    -   Pilates: 37

    -   Aerobics: 29

This analysis shows the distribution of exercise preferences among the 100 participants, providing insights into the most and least popular choices.

## Answers to Chapter 5 Practice Exercises {.unnumbered}

### Exercise 1: Importing Data {.unnumbered}

```{r, eval = F}
# Load necessary package
library(dplyr)

# Set working directory
setwd("path/to/your/folder")

# Import the CSV file
survey_data <- read.csv("survey_data.csv")

# View the first few rows of the data
head(survey_data)
```

```{r, eval = F}
# Install and load the readxl package
install.packages("readxl")
library(readxl)

# Import the Excel file
experiment_data <- read_excel("experiment_data.xlsx")

# View the first few rows of the data
head(experiment_data)

```

### Exercise 2: Cleaning Data with dplyr {.unnumbered}

```{r}
# Sample data
data <- data.frame(
  id = 1:10,
  age = c(23, 35, 42, NA, 30, 34, 21, 40, 29, 31),
  gender = c("M", "F", "F", "M", "M", "F", "M", "F", "M", "F"),
  score = c(80, 85, 78, 90, 85, 75, 88, 92, 84, NA)
)

# Remove rows with missing values
cleaned_data <- data %>%
  filter(!is.na(age) & !is.na(score)) %>%
  # Rename the age column
  rename(participant_age = age) %>%
  # Create a new column age_group
  mutate(age_group = ifelse(participant_age > 30, "Above 30", "30 or Below")) %>%
  # Remove outliers from the score column
  filter(score >= (quantile(score, 0.25) - 1.5 * IQR(score)) & score <= (quantile(score, 0.75) + 1.5 * IQR(score))) %>%
  # Relevel the age_group column
  mutate(age_group = relevel(factor(age_group), ref = "30 or Below"))

# View the cleaned data
print(cleaned_data)

```

**Interpretation**:

-   Rows with missing values in the `age` and `score` columns were removed.

-   The `age` column was renamed to `participant_age`.

-   A new column `age_group` was created, categorizing participants as "Above 30" or "30 or Below".

-   Outliers in the `score` column were removed using the IQR method.

-   The `age_group` column was re-leveled to set "30 or Below" as the reference level.

### Exercise 3: Generating Descriptive Statistics with `psych` {.unnumbered}

```{r, message = F, warning = F}
# Sample data
test_scores <- data.frame(
  id = 1:10,
  score = c(85, 90, 78, 92, 88, 76, 95, 89, 84, 91)
)

# Load the psych package
library(psych)

# Generate descriptive statistics
describe(test_scores)

```

**Interpretation**:

-   The `describe()` function provides a comprehensive summary of the `test_scores` dataset.

-   Mean: The average test score.

-   Standard Deviation: The variability of the test scores.

-   Skewness: The symmetry of the distribution.

-   Kurtosis: The peakedness of the distribution.

### Exercise 4: Visualizing Data with psych {.unnumbered}

```{r}
# Sample data
multi_var_data <- data.frame(
  score = c(85, 90, 78, 92, 88, 76, 95, 89, 84, 91),
  age = c(23, 25, 22, 24, 26, 21, 27, 25, 23, 24),
  study_hours = c(5, 6, 4, 6, 5, 3, 7, 6, 5, 6)
)

# Create the correlation plot
corMatrix <- cor(multi_var_data)
corPlot(corMatrix, numbers = TRUE, main = "Correlation Matrix")
```

**Interpretation**:

-   The correlation coefficients indicate the strength and direction of the relationships between variables.

-   Positive correlations: Variables increase together.

-   Negative correlations: One variable increases while the other decreases.

-   The numbers and colors help visualize these relationships.

```{r}
# Create the pair panels
pairs.panels(multi_var_data, 
             method = "pearson",  # correlation method
             hist.col = "blue",    # histogram color
             density = TRUE,       # add density plots
             ellipses = TRUE       # add correlation ellipses
)

```

**Interpretation**:

-   Scatterplots in the lower triangle show relationships between pairs of variables.

-   Histograms on the diagonal show the distribution of each variable.

-   Correlation coefficients in the upper triangle indicate the strength and direction of relationships.

-   Density plots add information about data concentration.

-   Correlation ellipses provide a visual representation of confidence intervals for the correlations.

## Answers to Chapter 6 Practice Exercises {.unnumbered}

### Exercise 1: Mean-Centering {.unnumbered}

**Dataset**: - `expenses <- c(1200, 1500, 1100, 1800, 1300, 1700, 1250, 1400, 1600, 1350)`

**Tasks and Answers**:

1.  **Calculate the mean of the expenses**:

```{r}
expenses <- c(1200, 1500, 1100, 1800, 1300, 1700, 1250, 1400, 1600, 1350)

mean_expenses <- mean(expenses)
mean_expenses
# Answer: 1425
```

2.  **Mean-center the dataset by subtracting the mean from each value**:

```{r}
   mean_centered_expenses <- expenses - mean_expenses
   mean_centered_expenses
   # Answer: -225, 75, -325, 375, -125, 275, -175, -25, 175, -75
```

3.  **Plot the original and mean-centered expenses on the same graph**:

```{r}
   y_limits <- range(c(expenses, mean_centered_expenses))
   plot(expenses, type = "b", col = "blue", ylab = "Expenses", xlab = "Month", main = "Original vs Mean-Centered Expenses", ylim = y_limits)
   lines(mean_centered_expenses, type = "b", col = "red")
   legend("topright", legend = c("Original", "Mean-Centered"), col = c("blue", "red"), lty = 1)
```

**Interpretation**:\
- **Answer**: A positive mean-centered value indicates that the expense for that month is above the average expense, while a negative value indicates that the expense is below the average. Mean-centering helps to visualize and analyze how each month's expense compares to the overall average.

### Exercise 2: Z-Scores {.unnumbered}

**Dataset**: - `test_scores <- c(65, 78, 82, 91, 70, 88, 75, 95, 80, 85)`

**Tasks and Answers**:

1.  **Calculate the mean and standard deviation of the test scores**:

```{r}
test_scores <- c(65, 78, 82, 91, 70, 88, 75, 95, 80, 85)
   mean_test_scores <- mean(test_scores)
   sd_test_scores <- sd(test_scores)
   mean_test_scores
   sd_test_scores
   # Answer: Mean = 79.9, Standard Deviation = 9.9
```

2.  **Compute the Z-scores for each test score**:

```{r}
   z_scores <- (test_scores - mean_test_scores) / sd_test_scores
   z_scores
   # Answer: -1.50, -0.19, 0.21, 1.11, -0.99, 0.82, -0.50, 1.53, 0.01, 0.51
```

3.  **Create a histogram of the Z-scores and add a vertical line at Z = 0**:

```{r}
   hist(z_scores, breaks = 10, col = "blue", xlab = "Z-Scores", main = "Histogram of Z-Scores")
   abline(v = 0, col = "red", lwd = 2)
```

**Interpretation**:\
- **Answer**: A Z-score greater than 0 indicates that the test score is above the average, while a Z-score less than 0 indicates that the test score is below the average. Z-scores help to standardize different test scores, making it easier to compare them. Outliers are typically identified as Z-scores beyond ±2 or ±3.

### Exercise 3: Combining Mean-Centering and Z-Scores {.unnumbered}

**Dataset**: - `reaction_times <- c(250, 340, 295, 310, 275, 325, 290, 360, 285, 310)`

**Tasks and Answers**:

1.  **Mean-center the reaction times**:

```{r}
reaction_times <- c(250, 340, 295, 310, 275, 325, 290, 360, 285, 310)
   mean_reaction_time <- mean(reaction_times)
   mean_centered_times <- reaction_times - mean_reaction_time
   mean_centered_times
   # Answer: -51, 39, -6, 9, -26, 24, -11, 59, -16, 9
```

2.  **Calculate the Z-scores for the mean-centered reaction times**:

```{r}
   sd_reaction_time <- sd(reaction_times)
   z_scores_centered <- mean_centered_times / sd_reaction_time
   z_scores_centered
   # Answer: -1.42, 1.08, -0.17, 0.25, -0.73, 0.68, -0.31, 1.68, -0.45, 0.25
```

3.  **Plot the original reaction times, mean-centered times, and Z-scores on separate graphs**:

```{r}
   par(mfrow = c(3, 1))
   plot(reaction_times, type = "b", col = "blue", ylab = "Reaction Times", xlab = "Index", main = "Original Reaction Times")
   plot(mean_centered_times, type = "b", col = "green", ylab = "Mean-Centered", xlab = "Index", main = "Mean-Centered Reaction Times")
   plot(z_scores_centered, type = "b", col = "red", ylab = "Z-Scores", xlab = "Index", main = "Z-Scores of Mean-Centered Reaction Times")
```

**Interpretation**:\
- **Answer**: Mean-centering adjusts the reaction times by subtracting the average, making it easier to see how each participant's time compares to the average. Z-scores take this a step further by standardizing the mean-centered times, showing how many standard deviations each time is from the mean. This combined approach helps in identifying outliers and comparing data points in a more meaningful way.

### Exercise 4: Non-Linear Transformations {.unnumbered}

**Dataset**: - `income <- c(30, 45, 70, 120, 25, 60, 100, 85, 40, 300)`

**Tasks and Answers**:

1.  **Apply a logarithmic transformation to the income data**:

```{r}
   income <- c(30, 45, 70, 120, 25, 60, 100, 85, 40, 300)
   log_income <- log(income)
   log_income
   # Answer: 3.40, 3.81, 4.25, 4.79, 3.22, 4.09, 4.61, 4.44, 3.69, 5.70
```

2.  **Apply a square root transformation to the income data**:

```{r}
   sqrt_income <- sqrt(income)
   sqrt_income
   # Answer: 5.48, 6.71, 8.37, 10.95, 5.00, 7.75, 10.00, 9.22, 6.32, 17.32
```

3.  **Apply an inverse transformation to the income data**:

```{r}
   inv_income <- 1 / income
   inv_income
   # Answer: 0.0333, 0.0222, 0.0143, 0.0083, 
```

4.  **Plot histograms of the original and transformed datasets**:

```{r}
   par(mfrow = c(2, 2))
   hist(income, breaks = 10, col = "blue", xlab = "Income", main = "Original Income")
   hist(log_income, breaks = 10, col = "green", xlab = "Log(Income)", main = "Log Transformed Income")
   hist(sqrt_income, breaks = 10, col = "orange", xlab = "Sqrt(Income)", main = "Square Root Transformed Income")
   hist(inv_income, breaks = 10, col = "red", xlab = "1/Income", main = "Inverse Transformed Income")
```

**Interpretation**:\
- **Answer**:\
- **Logarithmic Transformation**: Reduces skewness by pulling in large values, making the distribution more balanced. Useful when dealing with right-skewed data, such as income.\
- **Square Root Transformation**: Stabilizes variance, making the spread of the data more consistent across different values. Useful for data where variability increases with the value.\
- **Inverse Transformation**: Compresses large values, bringing them closer to smaller values. Useful when high values need to be reduced, such as in response times where quicker responses are more common.

## Answers to Chapter 7 Practice Exercises {.unnumbered}

### Exercise 1: Create an APA-Compliant Bar Graph {.unnumbered}

**Objective**: Create a bar graph comparing the mean values of a categorical variable, including error bars to represent variability.

**Solution**:

```{r, warning = F, message = F}
library(ggplot2)

# Create the APA-compliant bar graph
ggplot(mtcars, aes(x = factor(am), y = mpg)) +
  geom_bar(stat = "summary", fun = "mean", fill = "lightblue", color = "black") +
  geom_errorbar(stat = "summary", fun.data = "mean_se", width = 0.2, color = "black") +
  labs(title = "Average MPG by Transmission Type",
       x = "Transmission (0 = Automatic, 1 = Manual)",
       y = "Miles Per Gallon") +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(color = "black", size = 0.5, fill = NA)
  )
```

**Explanation**:\
- The `factor(am)` converts the transmission variable into a factor for categorical comparison.\
- `geom_bar()` creates the bar graph, while `geom_errorbar()` adds error bars representing the standard error of the mean.\
- APA formatting is applied using `theme_minimal()` with additional customization to meet APA standards.

### Exercise 2: Modify a Basic ggplot2 Plot to Meet APA Standards {.unnumbered}

**Objective**: Modify a basic scatter plot to adhere to APA formatting guidelines.

**Solution**:

```{r}
# Create the basic scatter plot
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point(size = 3) +
  labs(title = "Scatter Plot of Weight and MPG",
       x = "Weight (1000 lbs)",
       y = "Miles Per Gallon") +
  geom_smooth(method = "lm", se = FALSE, color = "black", linetype = "dashed", size = 0.7) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(color = "black", size = 0.5, fill = NA)
  )
```

**Explanation**:\
- A basic scatter plot is created with `geom_point()`.\
- A trend line is added using `geom_smooth(method = "lm", se = FALSE)`.\
- The plot is customized to meet APA standards by adjusting font sizes, adding a dashed trend line, and removing unnecessary grid lines.

### Exercise 3: Create an APA-Compliant Line Graph and Save It as a High-Resolution Image {.unnumbered}

**Objective**: Create a line graph comparing trends across groups, and save the graph as a high-resolution image.

**Solution**:

```{r}
# Create the APA-compliant line graph
p <- ggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +
  geom_line(size = 1, linetype = "solid") +
  labs(title = "MPG vs. Weight by Cylinder Count",
       x = "Weight (1000 lbs)",
       y = "Miles Per Gallon",
       color = "Cylinders") +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(color = "black", size = 0.5, fill = NA),
    legend.position = "right"
  )

# Save the graph as a high-resolution PNG file
ggsave("mpg_vs_weight_by_cyl.png", plot = p, width = 8, height = 6, dpi = 300)
```

**Explanation**:\
- The line graph is created using `geom_line()`, with different colors representing different cylinder counts.\
- APA formatting is applied using `theme_minimal()` with further customization for titles, axis labels, and legend placement.\
- The graph is saved as a high-resolution PNG file using `ggsave()` with specified dimensions and DPI to ensure print-quality resolution.

## Answers to Chapter 8 Practice Exercises {.unnumbered}

### Exercise 1: Standard Error Calculation and Interpretation

```{r}
# Standard deviations and sample sizes
sd_A <- 5
sd_B <- 6
n_A <- 30
n_B <- 30

# Calculate the standard error for the difference between means
SE_difference <- sqrt((sd_A^2 / n_A) + (sd_B^2 / n_B))
SE_difference
```

-   **Standard Error**: The standard error of the difference between the means was calculated as approximately 1.52.
-   **Interpretation**: This standard error suggests that the difference in sample means could vary by about 1.52 points due to random sampling variability. A smaller standard error would indicate more precise estimates of the true population means.

### Exercise 2: Confidence Interval Calculation and Interpretation {.unnumbered}

```{r}
# Means and standard error
mean_A <- 85
mean_B <- 80
SE_difference <- 2.5

# Calculate the 95% confidence interval
CI_lower <- (mean_A - mean_B) - 1.96 * SE_difference
CI_upper <- (mean_A - mean_B) + 1.96 * SE_difference
c(CI_lower, CI_upper)
```

-   **95% Confidence Interval**: The 95% confidence interval for the difference between the means was calculated as [0.1, 9.9].
-   **Interpretation**: This confidence interval suggests that the true difference in test scores between the two groups is likely between 0.1 and 9.9 points. Since the interval does not include zero, it supports the conclusion that there is a statistically significant difference between the two groups.

### Exercise 3: Independent Samples t-Test Interpretation {.unnumbered}

```{r}
# Sample data
therapy_A <- c(25, 30, 28, 34, 29, 31, 26, 32, 27, 33)
therapy_B <- c(22, 24, 26, 23, 27, 29, 25, 24, 26, 27)

# Conduct the t-test
t_test_result <- t.test(therapy_A, therapy_B, var.equal = TRUE)
t_test_result
```

-   **t-Value**: The t-value was calculated as 2.95.
-   **Degrees of Freedom**: The degrees of freedom were 18.
-   **p-Value**: The p-value was 0.008.
-   **Interpretation**: Since the p-value is less than 0.05, we reject the null hypothesis and conclude that there is a statistically significant difference between the two therapies in terms of reducing depression levels.

### Exercise 4: Paired Samples t-Test Interpretation {.unnumbered}

```{r}
# Sample data
before <- c(50, 45, 48, 53, 46, 47, 49, 44, 52, 50)
after <- c(40, 38, 42, 45, 39, 41, 40, 37, 44, 42)

# Conduct the paired t-test
paired_t_test_result <- t.test(before, after, paired = TRUE)
paired_t_test_result
```

-   **t-Value**: The t-value was calculated as 6.78.
-   **Degrees of Freedom**: The degrees of freedom were 9.
-   **p-Value**: The p-value was 0.0001.
-   **Interpretation**: The very low p-value suggests a significant reduction in anxiety levels after the mindfulness workshop. The large t-value indicates that the difference between pre- and post-intervention scores is substantial.

### Exercise 5: Significance and Effect Size Interpretation {.unnumbered}

-   **Statistical Significance**: The p-value of 0.04 indicates that the difference between the teaching methods is statistically significant at the 0.05 level.
-   **Effect Size (Cohen's d = 0.6)**: This medium effect size suggests that the difference between the teaching methods is not only statistically significant but also meaningful in practical terms. The teaching method has a moderate impact on student performance.
-   **Implications**: The study's findings suggest that the new teaching method is likely to result in better student performance, and the effect is both statistically and practically significant. The results may justify the adoption of the new method in educational settings.

## Answers to Chapter 9 Practice Exercises {.unnumbered}

### Exercise 1: Pearson Correlation Coefficient Calculation and Interpretation {.unnumbered}

```{r}
# Sample data
study_hours <- c(2, 4, 6, 8, 10)
exam_scores <- c(50, 55, 60, 65, 70)

# Calculate Pearson's correlation coefficient
correlation <- cor(study_hours, exam_scores)
correlation
```

-   **Calculated Pearson Correlation Coefficient**: 1
-   **Interpretation**: The correlation coefficient of 1 indicates a perfect positive correlation between study hours and exam scores. This suggests that as study hours increase, exam scores increase in a perfectly linear relationship.

### Exercise 2: Scatter Plot with Trend Line {.unnumbered}

```{r}
library(ggplot2)

# Sample data
study_hours <- c(2, 4, 6, 8, 10)
exam_scores <- c(50, 55, 60, 65, 70)

# Create scatter plot with trend line
ggplot(data = data.frame(study_hours, exam_scores), aes(x = study_hours, y = exam_scores)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Scatter Plot with Trend Line: Study Hours vs. Exam Scores", x = "Study Hours", y = "Exam Scores")
```

-   **Interpretation**: The scatter plot with a trend line shows a clear positive relationship between study hours and exam scores. The upward-sloping trend line indicates that higher study hours are associated with higher exam scores, consistent with the calculated Pearson correlation coefficient.

### Exercise 3: Analyzing the Size of the Correlation

-   **Interpretation**: The correlation coefficient of $r = 0.8$ indicates a strong positive relationship between the variables. In a psychological context, this suggests that study habits have a significant impact on academic performance. The large effect size implies that increasing study hours is likely to result in substantial improvements in exam scores, making it an important factor for students to consider.

### Exercise 4: Impact of a Third Variable (Confounder) and Controlling for It

-   **Discussion**: Loneliness could be influencing both social media use and anxiety levels, leading to a spurious correlation. To control for this confounding variable, future research could include loneliness as a covariate in statistical analyses or design an experiment where loneliness is manipulated or controlled.
-   **Suggestions**: Use methods such as multiple regression to control for loneliness, or conduct a longitudinal study to examine the temporal relationships between social media use, loneliness, and anxiety.

### Exercise 5: Evaluating Correlation and Causality {.unnumbered}

-   **Discussion**: The correlation between TV watching and obesity does not imply causality. It's possible that other factors, such as physical inactivity or dietary habits, are influencing both TV watching and obesity rates. Examples from the chapter, such as the correlation between ice cream sales and drowning rates, highlight the importance of not inferring causality from correlation alone.
-   **Examples**: Further research using experimental methods or longitudinal studies would be needed to establish whether TV watching directly contributes to obesity, or if other variables are at play.

## Answers to Chapter 10 Practice Exercises {.unnumbered}

### Exercise 1: Create a Simple Bivariate Linear Model {.unnumbered}

**Objective**: Create a bivariate linear model using the provided dataset, and interpret the slope, intercept, and residuals.

**Solution**:

```{r}
# Simulating the provided data
hours_studied <- c(2, 3, 5, 6, 8, 10)
exam_scores <- c(68, 72, 78, 85, 90, 95)

# Creating the linear model
model <- lm(exam_scores ~ hours_studied)

# Viewing the summary of the model
summary(model)

# Calculating the residuals
residuals <- residuals(model)

# Displaying residuals
residuals
```

**Interpretation**:

-   **Slope**: Suppose the model output shows that the slope (`b1`) is 3.5. This means that for every additional hour studied, the exam score is expected to increase by 3.5 points. This indicates a positive relationship between study time and exam performance.

-   **Intercept**: Let's say the intercept (`b0`) is 65. This suggests that if a student does not study at all (0 hours studied), their predicted exam score would be 65. The intercept provides a baseline score, representing the score a student might achieve without any study time.

-   **Residuals**: The residuals represent the differences between the observed exam scores and those predicted by the model. For example, if a student who studied for 6 hours scored 85, but the model predicted a score of 83, the residual would be 2 (85 - 83). If the residuals are small, it indicates that the model's predictions are close to the actual data. In this case, the residuals might be small, suggesting that the model fits the data well.

### Exercise 2: Analyze Residuals to Assess Model Fit {.unnumbered}

**Objective**: Analyze the residuals of a linear model to assess its fit and discuss any patterns you observe.

**Solution**:

```{r}
# Plotting the residuals
plot(hours_studied, residuals, main = "Residuals Plot",
     xlab = "Hours Studied", ylab = "Residuals", pch = 19, col = "blue")
abline(h = 0, col = "red", lwd = 2)
```

**Interpretation**:

-   **Random Distribution**: If the residuals are randomly scattered around the horizontal line at zero, this suggests that the model fits the data well, with no systematic errors. In this exercise, the residuals might appear to be randomly distributed, indicating that the linear model is appropriate for this dataset.

-   **Patterns**: If the residuals showed a pattern (e.g., they systematically increase or decrease), it might indicate that the model is not capturing some aspect of the data. For example, if residuals consistently increase as study hours increase, this could suggest a nonlinear relationship that a simple linear model cannot capture.

**Conclusion**: Assuming the residuals are randomly distributed in this scenario, you can conclude that the linear model is a good fit for the data. There are no evident patterns in the residuals, suggesting that the model appropriately captures the relationship between study hours and exam scores.

### Exercise 3: Apply Bivariate Linear Models to a Real-World Dataset {.unnumbered}

**Objective**: Apply what you've learned to analyze a real-world psychological dataset and interpret the results of your linear model.

**Solution**:

```{r}
# Simulating the dataset
set.seed(123)
anxiety <- rnorm(100, mean = 50, sd = 15)  # Anxiety scores (0 to 100)
sleep_hours <- 8 - 0.04 * anxiety + rnorm(100, mean = 0, sd = 1)  # Sleep hours

# Combining into a data frame
data <- data.frame(anxiety, sleep_hours)

# Creating the linear model
model <- lm(sleep_hours ~ anxiety, data = data)

# Viewing the summary of the model
summary(model)

# Plotting the residuals
residuals <- residuals(model)
plot(data$anxiety, residuals, main = "Residuals Plot",
     xlab = "Anxiety Levels", ylab = "Residuals", pch = 19, col = "blue")
abline(h = 0, col = "red", lwd = 2)
```

**Interpretation**:

-   **Slope**: Suppose the slope (`b1`) is -0.04. This suggests that for each additional point increase in anxiety score, the number of hours of sleep decreases by 0.04 hours (or approximately 2.4 minutes). This indicates a negative relationship between anxiety levels and sleep hours, meaning higher anxiety is associated with less sleep.

-   **Intercept**: If the intercept (`b0`) is 8, it represents the predicted number of sleep hours when anxiety is zero. This suggests that in the absence of anxiety, the expected sleep time is 8 hours.

-   **P-Value**: Let's assume the p-value associated with the slope is 0.02. Since this value is less than 0.05, it indicates that the relationship between anxiety levels and sleep hours is statistically significant. This means there is a meaningful association between higher anxiety and reduced sleep, not due to random chance.

-   **Residuals**: If the residuals are randomly distributed around the horizontal line at zero in the residuals plot, this suggests that the linear model is appropriate for this data. If you notice any patterns (e.g., a systematic curve), it might indicate that the model is not capturing the relationship correctly, and you might need to consider a more complex model.

**Conclusion**: The negative slope indicates that higher anxiety levels are associated with fewer hours of sleep. The statistically significant p-value supports this relationship, suggesting it is unlikely to have occurred by chance. The residuals plot confirms that the linear model is a good fit for the data, as the residuals appear randomly distributed with no apparent pattern. This analysis provides evidence that managing anxiety could be crucial for improving sleep quality.

## Answers to Chapter 11 Practice Exercises {.unnumbered}

### Exercise 1: Fit the Multiple Regression {.unnumbered}

```{r}
# Sample data
Study_Time <- c(10, 12, 9, 15, 8, 11, 7, 14, 10, 13)
Sleep_Quality <- c(7, 6, 8, 5, 7, 6, 7, 4, 8, 5)
Stress_Levels <- c(3, 5, 2, 6, 4, 5, 3, 7, 2, 6)
Academic_Performance <- c(85, 88, 80, 90, 75, 84, 78, 87, 82, 89)

# Fit the multiple regression model
model <- lm(Academic_Performance ~ Study_Time + Sleep_Quality + Stress_Levels)
summary(model)
```

-   **Main Effects**: The coefficients for `Study_Time`, `Sleep_Quality`, and `Stress_Levels` represent their unique contributions to predicting `Academic_Performance`.
    -   If `Study_Time` has a coefficient of 1.96, it means that for each additional hour of study, `Academic_Performance` increases by 1.96 points, holding other variables constant.

### Exercise 2: Compare Bivariate vs Multivariate {.unnumbered}

```{r}
# Sample data
Study_Time <- c(10, 12, 9, 15, 8, 11, 7, 14, 10, 13)
Stress_Levels <- c(3, 5, 2, 6, 4, 5, 3, 7, 2, 6)
Academic_Performance <- c(85, 88, 80, 90, 75, 84, 78, 87, 82, 89)

# Bivariate regression (Study_Time only)
model_bivariate <- lm(Academic_Performance ~ Study_Time)
summary(model_bivariate)

# Multiple regression with Stress_Levels
model_multiple <- lm(Academic_Performance ~ Study_Time + Stress_Levels)
summary(model_multiple)
```

-   **Suppression Effect**: If the coefficient for `Study_Time` increases after adding `Stress_Levels` to the model, it suggests that `Stress_Levels` was suppressing the true relationship between `Study_Time` and `Academic_Performance`.
    -   The increase in the coefficient indicates that `Study_Time` has a stronger relationship with `Academic_Performance` when accounting for `Stress_Levels`.

### Exercise 3: Plot Sleep Quality {.unnumbered}

```{r}
# Sample data
Sleep_Quality <- c(7, 6, 8, 5, 7, 6, 7, 4, 8, 5)
Study_Time <- c(10, 12, 9, 15, 8, 11, 7, 14, 10, 13)
Stress_Levels <- c(3, 5, 2, 6, 4, 5, 3, 7, 2, 6)
Academic_Performance <- c(85, 88, 80, 90, 75, 84, 78, 87, 82, 89)

# Fit the multiple regression model
model <- lm(Academic_Performance ~ Sleep_Quality + Study_Time + Stress_Levels)

# Create partial regression plot for Sleep_Quality
library(ggplot2)
ggplot(data.frame(Sleep_Quality, Academic_Performance), aes(x = Sleep_Quality, y = resid(lm(Academic_Performance ~ Study_Time + Stress_Levels)))) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Partial Regression Plot: Sleep Quality and Academic Performance",
       x = "Sleep Quality",
       y = "Residuals (Academic Performance)") +
  theme_minimal() +
  theme(text = element_text(size = 12),
        plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 10),
        panel.grid = element_blank(),
        panel.border = element_rect(color = "black", fill = NA))
```

-   **Interpretation of Plot**: The partial regression plot shows the relationship between `Sleep_Quality` and `Academic_Performance`, controlling for other variables. A positive trend line suggests that better sleep quality is associated with higher academic performance, even when controlling for study time and stress levels.

### Exercise 4: Bivariate vs Multivariate {.unnumbered}

```{r}
# Sample data
Sleep_Quality <- c(7, 6, 8, 5, 7, 6, 7, 4, 8, 5)
Study_Time <- c(10, 12, 9, 15, 8, 11, 7, 14, 10, 13)
Stress_Levels <- c(3, 5, 2, 6, 4, 5, 3, 7, 2, 6)
Academic_Performance <- c(85, 88, 80, 90, 75, 84, 78, 87, 82, 89)

# Bivariate regression
model_bivariate <- lm(Academic_Performance ~ Sleep_Quality)
summary(model_bivariate)

# Multiple regression
model_multiple <- lm(Academic_Performance ~ Sleep_Quality + Study_Time + Stress_Levels)
summary(model_multiple)
```

-   **Comparison of Bivariate and Multiple Regression**: The coefficient for `Sleep_Quality` may change when adding `Study_Time` and `Stress_Levels` to the model.
    -   If the coefficient for `Sleep_Quality` decreases, it suggests that `Study_Time` and `Stress_Levels` share some variance with `Sleep_Quality` in predicting `Academic_Performance`. This highlights the importance of including all relevant predictors in the model to avoid misleading conclusions.

## Answers to Chapter 12 Practice Exercises {.unnumbered}

### Exercise 1: Categorical x Categorical Interaction {.unnumbered}

-   **Task**: Create a model with a categorical x categorical interaction, interpret the interaction term, and visualize it using `ggplot2`.
-   **Instructions**:
    1.  Simulate a dataset with two categorical variables (e.g., Treatment: "A", "B" and Gender: "Male", "Female") and an outcome variable (e.g., Recovery Rate).
    2.  Fit a linear model that includes an interaction term between the two categorical variables.
    3.  Interpret the interaction term in the context of the outcome variable.
    4.  Visualize the interaction using a bar graph with error bars.

```{r, warning = F, message = F}
# Simulate data
set.seed(123)
Treatment <- factor(rep(c("A", "B"), each = 50))
Gender <- factor(rep(c("Male", "Female"), each = 25, times = 2))
Recovery_Rate <- ifelse(Treatment == "A", 80 + 5 * (Gender == "Male"), 
                        70 + 10 * (Gender == "Female")) + rnorm(100, sd = 5)

data <- data.frame(Treatment, Gender, Recovery_Rate)

# Fit the model
model <- lm(Recovery_Rate ~ Treatment * Gender, data = data)

# Summary of the model
summary(model)

# Visualize the interaction
library(ggplot2)
library(dplyr)

# Calculate group means and standard errors
group_summary <- data %>%
  group_by(Treatment, Gender) %>%
  summarise(
    Mean_Recovery_Rate = mean(Recovery_Rate),
    SE_Recovery_Rate = sd(Recovery_Rate) / sqrt(n())
  )

# Bar graph with error bars
ggplot(group_summary, aes(x = Treatment, y = Mean_Recovery_Rate, fill = Gender)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.9), width = 0.7) +
  geom_errorbar(aes(ymin = Mean_Recovery_Rate - SE_Recovery_Rate, ymax = Mean_Recovery_Rate + SE_Recovery_Rate),
                position = position_dodge(width = 0.9), width = 0.25) +
  labs(title = "Interaction between Treatment and Gender on Recovery Rate",
       x = "Treatment", y = "Mean Recovery Rate (%)") +
  theme_minimal()
```

-   **Interpretation**: The interaction term indicates how the effect of treatment on recovery rate differs by gender. For example, if the interaction term is significant, it may suggest that Treatment A is more effective for males while Treatment B is more effective for females.

### Exercise 2: Linear x Linear Interaction {.unnumbered}

-   **Task**: Model a linear x linear interaction, interpret the coefficients, and create a graph to visualize the interaction.
-   **Instructions**:
    1.  Simulate a dataset with two continuous variables (e.g., Age and Experience) and an outcome variable (e.g., Salary).
    2.  Fit a linear model that includes an interaction term between the two continuous variables.
    3.  Interpret the coefficients, especially the interaction term.
    4.  Create a 3D surface plot to visualize the interaction or use a 2D plot with a median split.

```{r}
# Simulate data
set.seed(123)
Age <- rnorm(100, mean = 40, sd = 10)
Experience <- rnorm(100, mean = 15, sd = 5)
Salary <- 30000 + 1000 * Age + 2000 * Experience + 150 * Age * Experience + rnorm(100, sd = 5000)

data <- data.frame(Age, Experience, Salary)

# Fit the model
model <- lm(Salary ~ Age * Experience, data = data)

# Summary of the model
summary(model)

# 2D plot using median split
data <- data %>%
  mutate(Experience_Level = ifelse(Experience > median(Experience), "High", "Low"))

ggplot(data, aes(x = Age, y = Salary, color = Experience_Level)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Effect of Age on Salary by Experience Level",
       x = "Age", y = "Salary") +
  theme_minimal()
```

-   **Interpretation**: The interaction term (Age:Experience) represents how the effect of Age on Salary changes depending on the level of Experience. A significant interaction would suggest that the relationship between Age and Salary is different for those with higher versus lower levels of experience.

### Exercise 3: Categorical x Linear Interaction {.unnumbered}

-   **Task**: Model a categorical x linear interaction, interpret the results, and create an interaction plot to illustrate the relationship.
-   **Instructions**:
    1.  Simulate a dataset with one categorical variable (e.g., Gender) and one continuous variable (e.g., Hours of Study) affecting an outcome variable (e.g., Test Scores).
    2.  Fit a linear model that includes an interaction term between the categorical and continuous variables.
    3.  Interpret the results, focusing on the interaction term.
    4.  Create an interaction plot using `ggplot2` to visualize the interaction.

```{r}
# Simulate data
set.seed(123)
Gender <- factor(rep(c("Male", "Female"), each = 50))
Hours_Study <- rnorm(100, mean = 5, sd = 2)
Test_Score <- 70 + 5 * Hours_Study + 10 * (Gender == "Female") + 5 * Hours_Study * (Gender == "Female") + rnorm(100, sd = 5)

data <- data.frame(Gender, Hours_Study, Test_Score)

# Fit the model
model <- lm(Test_Score ~ Gender * Hours_Study, data = data)

# Summary of the model
summary(model)

# Interaction plot
ggplot(data, aes(x = Hours_Study, y = Test_Score, color = Gender)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Interaction between Gender and Hours of Study on Test Scores",
       x = "Hours of Study", y = "Test Score") +
  theme_minimal()
```

-   **Interpretation**: The interaction term (Gender:Hours_Study) tells us how the effect of study hours on test scores differs between males and females. If significant, it suggests that the relationship between study hours and test scores is stronger or weaker depending on gender.

### Exercise 4: Graphing Multivariate Interactions {.unnumbered}

-   **Task**: Given a multivariate dataset, create different types of graphs to visualize interactions and discuss which type of graph is most appropriate.
-   **Instructions**:
    1.  Use a provided dataset (or simulate one) with multiple predictors (both continuous and categorical) and an outcome variable.
    2.  Create various types of graphs (e.g., interaction plots, 3D surface plots, faceted plots).
    3.  Discuss which type of graph best represents the interactions in your data and why.

```{r}
# Simulate a multivariate dataset
set.seed(123)
Age <- rnorm(100, mean = 40, sd = 10)
Experience <- rnorm(100, mean = 15, sd = 5)
Gender <- factor(rep(c("Male", "Female"), each = 50))
Salary <- 30000 + 1000 * Age + 2000 * Experience + 150 * Age * Experience + 5000 * (Gender == "Female") + rnorm(100, sd = 5000)

data <- data.frame(Age, Experience, Gender, Salary)

# Interaction plot (categorical x continuous)
ggplot(data, aes(x = Age, y = Salary, color = Gender)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Interaction between Gender and Age on Salary",
       x = "Age", y = "Salary") +
  theme_minimal()

# 3D surface plot (linear x linear interaction)
library(rgl)

age_grid <- seq(min(data$Age), max(data$Age), length.out = 30)
experience_grid <- seq(min(data$Experience), max(data$Experience), length.out = 30)

age_matrix <- outer(age_grid, rep(1, length(experience_grid)))
experience_matrix <- outer(rep(1, length(age_grid)), experience_grid)

salary_pred <- outer(age_grid, experience_grid, 
                     function(a, e) 30000 + 1000 * a + 2000 * e + 150 * a * e)

plot3d(age_matrix, experience_matrix, salary_pred, col = "lightblue", alpha = 0.7, type = "n")
points3d(data$Age, data$Experience, data$Salary, col = "blue", size = 3)
surface3d(age_matrix, experience_matrix, salary_pred, color = "lightblue", alpha = 0.5)
rglwidget()

# Faceted plot (continuous x categorical interaction)
ggplot(data, aes(x = Experience, y = Salary)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Effect of Experience on Salary by Gender",
       x = "Experience", y = "Salary") +
  facet_wrap(~ Gender) +
  theme_minimal()
```

-   **Discussion**:
    -   The **interaction plot** is best for showing how a continuous variable interacts with a categorical variable.

## Answers to Chapter 13 Practice Exercises {.unnumbered}


### Exercise 1: Fitting a Logistic Regression Model {.unnumbered}
**Task**: Using the provided dataset, fit a logistic regression model to predict whether a person survived the Titanic disaster (`Survived`), based on the predictors `Sex`, `Age`, and `Pclass`. Interpret the exponentiated coefficients (odds ratios) for each predictor.

```{r, warning = F, message = F}
# Load necessary packages
library(dplyr)

# Generate a new example dataset with significant effects
set.seed(123)
titanic_data <- data.frame(
  Survived = rbinom(800, 1, prob = 0.5),
  Sex = factor(sample(c("Male", "Female"), 800, replace = TRUE)),
  Age = sample(0:95, 800, replace = TRUE),
  Pclass = factor(sample(1:3, 800, replace = TRUE), levels = c("1", "2", "3"))
)

# Adjust the dataset to create significant relationships
titanic_data$Survived[titanic_data$Age > 10] <- rbinom(sum(titanic_data$Age > 10), 1, prob = 0.1)
titanic_data$Survived[titanic_data$Age <= 10] <- rbinom(sum(titanic_data$Age <= 10), 1, prob = 0.9)
titanic_data$Survived[titanic_data$Sex == "Female"] <- rbinom(sum(titanic_data$Sex == "Female"), 1, prob = 0.7)
titanic_data$Survived[titanic_data$Pclass == "1"] <- rbinom(sum(titanic_data$Pclass == "1"), 1, prob = 0.8)
titanic_data$Survived[titanic_data$Pclass == "3"] <- rbinom(sum(titanic_data$Pclass == "3"), 1, prob = 0.2)

# Ensure Pclass has "1" as the reference level
titanic_data$Pclass <- relevel(titanic_data$Pclass, ref = "1")

# Fit the logistic regression model
model <- glm(Survived ~ Sex + Age + Pclass, data = titanic_data, family = binomial)

# Exponentiate coefficients to get odds ratios
odds_ratios <- exp(coef(model))
conf_int <- exp(confint(model))

# Display the results
summary(model)
odds_ratios
conf_int

```
- **Interpretation**:
  - **Sex (Female vs. Male)**: Females have higher odds of surviving compared to males.
  - **Age**: Older age  decreases the odds of survival.
  - **Pclass (1st vs. 2nd/3rd)**: Passengers in 1st class have significantly higher odds of survival compared to those in 2nd or 3rd class.

###  Exercise 2: Visualizing Logistic Regression Results {.unnumbered}
**Task**: Create a plot to visualize the predicted probabilities of survival (`Survived`) based on `Age`. Use the `ggplot2` package to plot the logistic regression curve.

```{r}
# Load necessary packages
library(ggplot2)

# Generate predicted probabilities
titanic_data$predicted_prob <- predict(model, newdata = titanic_data, type = "response")

# Plot the logistic regression curve
ggplot(titanic_data, aes(x = Age, y = predicted_prob)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "glm", method.args = list(family = binomial), color = "blue") +
  labs(title = "Predicted Probability of Survival by Age",
       x = "Age", y = "Predicted Probability") +
  theme_minimal()
```

  - The logistic regression curve shows that as age increases, the predicted probability of survival decreases, illustrating the negative impact of age on survival.
  
### Exercise 3: Interpreting Odds Ratios {.unnumbered}
**Task**: Interpret the odds ratios obtained in Exercise 1. Specifically, discuss the practical significance of the odds ratios for `Sex`, `Age`, and `Pclass` in predicting survival on the Titanic.

```{r}
# Odds ratios interpretation (example text)
# Odds ratio for Sex (Female vs. Male): If the odds ratio for 'Female' is 2.5, it means that females were 2.5 times more likely to survive compared to males, holding all other factors constant.

```

- **Odds Ratios**:
  - **Sex**: An odds ratio greater than 1 for females suggests they were more likely to survive than males.
  - **Age**: A value slightly less than 1 indicates that increasing age decreases survival odds.
  - **Pclass**: Higher class status increases the likelihood of survival.
  
### Exercise 4: Checking Model Fit {.unnumbered}
**Task**: Assess the fit of the logistic regression model you fitted in Exercise 1. Plot an ROC curve. Discuss the ROC.

```{r}
# Load necessary packages
library(ResourceSelection)
library(pROC)

# ROC curve
roc_curve <- roc(titanic_data$Survived, titanic_data$predicted_prob)

# Plot ROC curve
plot(roc_curve, col = "blue", main = "ROC Curve for Titanic Survival Model")
auc(roc_curve)


```

- **ROC Curve**: AUC value closer to 1 indicates good model performance. For example, an AUC of 0.85 suggests good discriminative ability.


## Answers to Chapter 14 Practice Exercises {.unnumbered}

### Exercise 1: Chi-Square Goodness of Fit Test {.unnumbered}

```{r}
# Observed and expected frequencies
observed <- c(40, 35, 25)
expected <- c(33.3, 33.3, 33.3)

# Perform Chi-Square test
chi_square_test <- chisq.test(observed, p = expected / sum(expected))
chi_square_test
```

**Interpretation**:\
- **Chi-Square Statistic**: The test will output the Chi-Square statistic. A large value suggests that the observed frequencies differ significantly from the expected frequencies.\
- **P-Value**: If the p-value is less than 0.05, reject the null hypothesis and conclude that the observed frequencies are significantly different from the expected frequencies.

### Exercise 2: Calculating R-Squared {.unnumbered}

```{r}
# Data for regression analysis
study_hours <- c(4, 6, 8, 10, 12, 14, 16, 18, 20)
exam_scores <- c(55, 60, 65, 70, 75, 80, 85, 90, 95)

# Fit linear regression model
model <- lm(exam_scores ~ study_hours)

# Calculate R-squared
summary(model)$r.squared
```

**Interpretation**:\
- **R-Squared**: If R-squared is close to 1, it indicates that the model explains a large proportion of the variance in exam scores. If it's closer to 0, the model explains very little of the variance.

### Exercise 3: F-Test for Comparing Models {.unnumbered}

```{r}
# Data for regression analysis
hours_of_sleep <- c(5, 6, 7, 8, 5, 6, 7, 8, 9)
caffeine_intake <- c(3, 2, 4, 5, 2, 3, 5, 6, 7)
reaction_time <- c(12, 10, 9, 8, 13, 11, 10, 9, 7)

# Fit simple model
model1 <- lm(reaction_time ~ hours_of_sleep)

# Fit more complex model
model2 <- lm(reaction_time ~ hours_of_sleep + caffeine_intake)

# Perform F-test to compare models
anova(model1, model2)
```

**Interpretation**:\
- **F-Statistic**: A higher F-statistic indicates that the more complex model explains significantly more variance than the simpler model.\
- **P-Value**: If the p-value is less than 0.05, the more complex model provides a significantly better fit.

### Exercise 4: Visualizing the F-Distribution {.unnumbered}

```{r}
# Load ggplot2
library(ggplot2)

# Define F-values
f_values <- seq(0, 5, length.out = 100)

# Plot F-distribution for different degrees of freedom
plot_df <- data.frame(
  F_Values = f_values,
  DF1_5_DF2_10 = df(f_values, df1 = 5, df2 = 10),
  DF1_10_DF2_20 = df(f_values, df1 = 10, df2 = 20)
)

ggplot(plot_df, aes(x = F_Values)) +
  geom_line(aes(y = DF1_5_DF2_10, color = "DF1 = 5, DF2 = 10")) +
  geom_line(aes(y = DF1_10_DF2_20, color = "DF1 = 10, DF2 = 20")) +
  labs(title = "F-Distribution for Different Degrees of Freedom",
       x = "F-Value",
       y = "Density",
       color = "Degrees of Freedom") +
  theme_minimal()
```

**Interpretation**:\
- The shape of the F-distribution changes depending on the degrees of freedom. More degrees of freedom result in a distribution that is closer to normal, with a higher peak and narrower spread. This influences the critical value used in the F-test, making it easier or harder to achieve statistical significance.

### Exercise 5: Comprehensive Analysis {.unnumbered}

**Step 1: Chi-Square Test**

```{r}
observed_frequencies <- c(30, 45, 25)
expected_frequencies <- c(33.3, 33.3, 33.3)

chi_square_test <- chisq.test(observed_frequencies, p = expected_frequencies / sum(expected_frequencies))
chi_square_test
```

**Step 2: R-Squared Calculation**

```{r}
study_hours <- c(3, 5, 7, 9, 11, 13, 15, 17, 19)
exam_scores <- c(50, 55, 60, 65, 70, 75, 80, 85, 90)

model <- lm(exam_scores ~ study_hours)
summary(model)$r.squared
```

**Step 3: F-Test**

```{r}
stress_levels <- c(7, 8, 6, 9, 7, 8, 6, 7, 9)
social_support <- c(5, 6, 4, 8, 5, 6, 7, 8, 9)

model1 <- lm(stress_levels ~ study_hours)
model2 <- lm(stress_levels ~ study_hours + social_support)

anova(model1, model2)
```

**Interpretation**: Write a comprehensive report summarizing the findings from the Chi-Square test, R-squared calculation, and F-test. Discuss how these results contribute to understanding the factors influencing the dependent variables and the overall goodness of fit for the models.

## Answers to Chapter 15 Practice Exercises {.unnumbered}

### Exercise 1: Power Calculation Interpretation {.unnumbered}

-   The power calculation output will show a power value (e.g., 0.73). If the power is less than 0.80, the study is underpowered, meaning there is a higher risk of not detecting a true effect. Ideally, the study should have a power of at least 0.80 to ensure a reasonable chance of detecting the expected effect.

### Exercise 2: Sample Size Calculation Interpretation {.unnumbered}

-   The output will show the required sample size per group (e.g., 64). This sample size is necessary to achieve 80% power, meaning that with this number of participants, you have an 80% chance of detecting a true effect if it exists.

### Exercise 3: Type I Error Analysis Interpretation {.unnumbered}

-   **Potential Consequences**: Publishing a false positive can mislead other researchers, lead to ineffective interventions being adopted in clinical practice, and damage the credibility of the field.\
-   **Improvement Suggestions**: The study design could have been improved by using a more stringent significance level (e.g., 0.01) or increasing the sample size to ensure that any detected effect was more likely to be genuine.

### Exercise 4: Type II Error Analysis Interpretation {.unnumbered}

-   **Potential Consequences**: Missing a true effect means that a potentially effective therapy might be dismissed, depriving patients of a beneficial treatment. This could delay advancements in treatment for PTSD.\
-   **Strategies to Increase Power**: Increasing the sample size, using more sensitive measures, and conducting a thorough power analysis before the study could help reduce the risk of Type II errors.

### Exercise 5: Power Comparison Interpretation {.unnumbered}

-   **Scenario 1**: With a small effect size, small sample size, and alpha = 0.05, the power is likely to be very low (e.g., 0.20), indicating a high risk of Type II error.\
-   **Scenario 2**: With a moderate effect size and medium sample size, the power should be around 0.80, which is adequate for detecting the effect.\
-   **Scenario 3**: With a large effect size, large sample size, and lower alpha, the power will be very high (e.g., 0.90 or higher), indicating a strong likelihood of detecting a true effect if it exists.

------------------------------------------------------------------------

This appendix will be continuously updated as new exercises and chapters are added to the textbook, providing a comprehensive resource for students to check their work and ensure they understand the material thoroughly.


::: foreword
# Appendix: Answers to Chapter Exercises

This appendix provides solutions to the exercises given at the end of each chapter. These solutions are intended to help you verify your work and understand the correct approach to each task.

## Answers to Chapter 1 Exercises

### Exercise 1: Familiarization with R Studio

1.  **Create a new R script and save it**
    -   Open R Studio, go to `File > New File > R Script`. This will open a new script tab in the Source Pane.
    -   Save the script by clicking `File > Save As...`, and name it `practice_script.R`.
2.  **Write and run a simple calculation**
    -   In the script, write the following line of code:

        ``` r
        8 * 9
        ```

    -   To run this line, place your cursor on the line and press `Ctrl + Enter` (Windows) or `Cmd + Enter` (macOS).
3.  **Comment your code**
    -   Add a comment above the code explaining what it does:

        ``` r
        # This code calculates the product of 8 and 9
        8 * 9
        ```

### Exercise 2: Basic Data Entry and Operation

1.  **Create a vector of numbers**
    -   Write the following line in an R script to create the vector:

        ``` r
        numbers <- 1:10
        ```
2.  **Calculate the sum of the vector**
    -   To calculate and print the sum, add this line to your script:

        ``` r
        print(sum(numbers))
        ```
3.  **Save the script**
    -   Ensure your work is saved in the script `practice_script.R` or in a new script file if preferred.

### Exercise 3: Introduction to R Markdown

1.  **Create a new R Markdown document**
    -   Go to `File > New File > R Markdown...`, provide a title "My First R Markdown", and fill in your name as the author.
2.  **Write a brief introduction**
    -   In the document, use the following Markdown syntax:

        ``` markdown
        # About Me
        This is a brief introduction about myself.
        - I am learning R and R Studio.
        - I enjoy data analysis.
        **Bold Fact**: I aim to be a data scientist.
        ```
3.  **Embed a chunk of R code**
    -   Include a code chunk that calculates the square of 12:

```{r}
12^2
```

4.  **Knit the document to HTML**
    -   Click the `Knit` button and select `Knit to HTML`. Save the output in your project directory.

### Exercise 4: Exploring the Help Pane

1.  **Find help on the `plot` function**
    -   In the Console, type `?plot` and press Enter. Review the help file that appears in the Help pane.
2.  **Write a command to plot a graph**
    -   In an R script, add the following line to plot a graph:

        ``` r
        plot(1:10, 1:10)
        ```
3.  **Add a title to the plot**
    -   Modify the plot command to include a title:

        ``` r
        plot(1:10, 1:10, main = "Simple Linear Plot")
        ```

## Answers to Chapter 2 Exercises

### Answers to Exercise 1: Identifying Data Types

1.  **Scenario Analysis**:
    -   **Children at playground**: The data collection method used here is **observational data**. The psychologist is observing natural behaviors without intervening or manipulating the environment.
    -   **Evening diary entries**: This scenario uses **self-report data** as participants are providing personal accounts of their feelings and activities.
    -   **Noise level manipulation**: This is an example of **experimental manipulation**, where a variable (noise level) is deliberately changed to observe its effect on another variable (productivity).

### Answers to Exercise 2: Designing a Study

1.  **Study Design**:
    -   **Research Question**: Does listening to classical music while studying improve memory recall?
    -   **Type of Data**: Experimental manipulation.
    -   **Data Collection Method**: Participants are randomly assigned to two groups. One group studies in silence while the other listens to classical music. Afterwards, both groups take a memory test based on the material studied.
    -   **Ethical Considerations**: Ensure that participants are aware they can withdraw at any time and that all data collected will be confidential. Consider any potential stress or anxiety induced by test conditions and address these in the study design.

### Answers to Exercise 3: Evaluating Research

1.  **Research Evaluation**:
    -   **Type of Data Used**: Assuming the study involves assessing the effects of sleep on cognitive performance using different sleep interventions, the data type would likely be **experimental manipulation**.
    -   **Potential Biases**: If the study does not adequately randomize participants or control for other factors affecting sleep (like caffeine intake or room conditions), results could be biased.
    -   **Influence on Conclusions**: The use of experimental manipulation allows the researcher to make stronger causal claims about the effect of sleep on cognitive performance compared to observational or self-report data. However, biases and experimental design flaws can undermine these claims.

## Answers to Chapter 3 Exercises

### Answers to Exercise 1: Evaluating Reliability

1.  **Scenario Analysis**:
    -   **Answer**: The Pearson correlation coefficient of 0.65 indicates moderate test-retest reliability. While this isn't considered low, for measures of psychological constructs such as self-esteem, a higher coefficient (typically 0.7 or above) is generally preferred to ensure consistency over time. A coefficient of 0.65 might suggest that the questionnaire could benefit from further refinement to improve reliability.

### Answers to Exercise 2: Assessing Validity

1.  **Scenario Development**:
    -   **Answer**: Steps to validate the aptitude test could include:
        -   **Developing a Hypothesis**: Predict that high scores on the aptitude test correlate with higher academic performance in college.
        -   **Collecting Data**: Gather test scores from incoming college students and their subsequent grade point averages (GPAs) at the end of their first year.
        -   **Statistical Analysis**: Perform a correlation analysis to assess the relationship between test scores and GPAs.
        -   **Interpreting Results**: A strong positive correlation would indicate good predictive validity of the aptitude test for college success.

### Answers to Exercise 3: Identifying and Addressing Data Collection Errors

1.  **Problem Solving**:
    -   **Answer**: The miscalibration of the sleep quality device could lead to inaccurate data, potentially skewing the study results. To mitigate this impact:
        -   **Re-calibrate the device**: Immediately correct the calibration error for future data collection.
        -   **Analyze impacted data**: Assess the extent of the data affected by the miscalibration and consider excluding or adjusting this data in the analysis.
        -   **Transparency in Reporting**: Disclose the issue and the steps taken to address it in any publications or presentations involving this research.

### Answers to Exercise 4: Triangulation to Enhance Validity

1.  **Critical Thinking**:
    -   **Answer**: Using multiple data sources like surveys, observations, and performance metrics helps enhance the construct validity of the study. This triangulation approach allows for validation of the findings through different perspectives, reducing the bias that might be present if only one method were used. Each method complements the others, providing a more holistic view of student engagement.

### Answers to Exercise 5: Role Play on Ethical Data Collection

1.  **Discussion**:
    -   **Answer**: Key procedures and safeguards might include:
        -   **Informed Consent**: Ensure all participants are fully aware of the nature of the data being collected and its intended use. Obtain written consent.
        -   **Anonymity and Confidentiality**: Assign codes to participants instead of using names and store personal data securely. Ensure that any reports or publications do not allow individual participants to be identified.
        -   **Minimizing Harm**: Be sensitive to how questions about personal health might affect participants and provide support resources as necessary.

### Answers to Exercise 6: Real-World Application

1.  **Application**:
    -   **Answer**: This exercise is subjective and would depend on the specific study chosen. Generally, the answer should include an evaluation of the methods section for clarity on measurement tools, reliability coefficients, validity assertions, and a discussion on how well the study accounted for potential data collection errors. Suggestions for improvement might include more rigorous reliability testing, additional validation studies, or enhanced error checking procedures.

## Answers to Chapter 4 Practice Exercises

### Exercise 1: Calculating Descriptive Statistics

**Dataset**: `c(55, 65, 75, 85, 95, 105, 115, 125, 135, 145)`

```{r}
# Sample data vector
scores <- c(55, 65, 75, 85, 95, 105, 115, 125, 135, 145)

# Calculate mean
mean_score <- mean(scores)
print(paste("Mean:", mean_score))

# Calculate median
median_score <- median(scores)
print(paste("Median:", median_score))

# Calculate mode
get_mode <- function(x) {
  uniqv <- unique(x)
  uniqv[which.max(tabulate(match(x, uniqv)))]
}
mode_score <- get_mode(scores)
print(paste("Mode:", mode_score))

# Calculate variance
variance_value <- var(scores)
print(paste("Variance:", variance_value))

# Calculate standard deviation
std_deviation <- sd(scores)
print(paste("Standard Deviation:", std_deviation))

# Identify outliers using IQR
Q1 <- quantile(scores, 0.25)
Q3 <- quantile(scores, 0.75)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
outliers <- scores[scores < lower_bound | scores > upper_bound]

print(paste("Outliers:", paste(outliers, collapse = ", ")))

```

**Interpretation**:

-   **Mean**: 100

-   **Median**: 100

-   **Mode**: Since all values are unique, there is no mode in this dataset.

-   **Variance**: 1100

-   **Standard Deviation**: 33.16625

-   **Outliers**: There are no outliers in this dataset as all values lie within the lower and upper bounds.

### Exercise 2: Understanding the Normal Distribution

Assume a psychological test follows a normal distribution with a mean of 100 and a standard deviation of 15.

```{r}
# Parameters
mean <- 100
sd <- 15

# Probability of a score less than 85
prob_less_than_85 <- pnorm(85, mean, sd)
print(paste("Probability of a score less than 85:", prob_less_than_85))

# Probability of a score between 85 and 115
prob_between_85_and_115 <- pnorm(115, mean, sd) - pnorm(85, mean, sd)
print(paste("Probability of a score between 85 and 115:", prob_between_85_and_115))

```

**Interpretation**:

-   **Probability of a score less than 85**: 0.1586553 (or 15.87%)

-   **Probability of a score between 85 and 115**: 0.6826895 (or 68.27%)

### Exercise 3: Applying the T-Distribution

You are conducting a small-scale study with 12 participants.

```{r}
# Degrees of freedom
df <- 11  # for n = 12, df = n - 1

# Probability of a t-score less than 1.5
prob_less_than_1_5 <- pt(1.5, df)
print(paste("Probability of a t-score less than 1.5:", prob_less_than_1_5))

# Probability of a t-score between -1 and 1
prob_between_minus1_and_1 <- pt(1, df) - pt(-1, df)
print(paste("Probability of a t-score between -1 and 1:", prob_between_minus1_and_1))
```

**Interpretation**:

-   **Probability of a t-score less than 1.5**: 0.9180312 (or 91.80%)

-   **Probability of a t-score between -1 and 1**: 0.5764421 (or 57.64%)

### Exercise 4: Defining and Simulating Sample Spaces

Define a sample space for a study where participants can choose between three types of exercises (Yoga, Pilates, Aerobics). Simulate responses from 100 participants.

```{r}
# Define the sample space
sample_space <- c("Yoga", "Pilates", "Aerobics")

# Simulate responses from 100 participants
set.seed(123)  # For reproducibility
responses <- sample(sample_space, 100, replace = TRUE)

# Display the first 10 responses
print(responses[1:10])

# Analyze the frequency of each exercise choice
exercise_frequency <- table(responses)
print(exercise_frequency)
```

**Interpretation**:

-   **Sample Space**: {Yoga, Pilates, Aerobics}

-   **Simulated Responses (first 10)**: ["Pilates", "Yoga", "Yoga", "Yoga", "Aerobics", "Yoga", "Yoga", "Yoga", "Pilates", "Yoga"]

-   **Frequency Analysis**:

    -   Yoga: 34

    -   Pilates: 37

    -   Aerobics: 29

This analysis shows the distribution of exercise preferences among the 100 participants, providing insights into the most and least popular choices.

## Answers to Chapter 5 Practice Exercises

### Exercise 1: Importing Data

```{r, eval = F}
# Load necessary package
library(dplyr)

# Set working directory
setwd("path/to/your/folder")

# Import the CSV file
survey_data <- read.csv("survey_data.csv")

# View the first few rows of the data
head(survey_data)
```

```{r, eval = F}
# Install and load the readxl package
install.packages("readxl")
library(readxl)

# Import the Excel file
experiment_data <- read_excel("experiment_data.xlsx")

# View the first few rows of the data
head(experiment_data)

```

### Exercise 2: Cleaning Data with dplyr

```{r}
# Sample data
data <- data.frame(
  id = 1:10,
  age = c(23, 35, 42, NA, 30, 34, 21, 40, 29, 31),
  gender = c("M", "F", "F", "M", "M", "F", "M", "F", "M", "F"),
  score = c(80, 85, 78, 90, 85, 75, 88, 92, 84, NA)
)

# Remove rows with missing values
cleaned_data <- data %>%
  filter(!is.na(age) & !is.na(score)) %>%
  # Rename the age column
  rename(participant_age = age) %>%
  # Create a new column age_group
  mutate(age_group = ifelse(participant_age > 30, "Above 30", "30 or Below")) %>%
  # Remove outliers from the score column
  filter(score >= (quantile(score, 0.25) - 1.5 * IQR(score)) & score <= (quantile(score, 0.75) + 1.5 * IQR(score))) %>%
  # Relevel the age_group column
  mutate(age_group = relevel(factor(age_group), ref = "30 or Below"))

# View the cleaned data
print(cleaned_data)

```

**Interpretation**:

-   Rows with missing values in the `age` and `score` columns were removed.

-   The `age` column was renamed to `participant_age`.

-   A new column `age_group` was created, categorizing participants as "Above 30" or "30 or Below".

-   Outliers in the `score` column were removed using the IQR method.

-   The `age_group` column was re-leveled to set "30 or Below" as the reference level.

### Exercise 3: Generating Descriptive Statistics with `psych`

```{r, message = F, warning = F}
# Sample data
test_scores <- data.frame(
  id = 1:10,
  score = c(85, 90, 78, 92, 88, 76, 95, 89, 84, 91)
)

# Load the psych package
library(psych)

# Generate descriptive statistics
describe(test_scores)

```

**Interpretation**:

-   The `describe()` function provides a comprehensive summary of the `test_scores` dataset.

-   Mean: The average test score.

-   Standard Deviation: The variability of the test scores.

-   Skewness: The symmetry of the distribution.

-   Kurtosis: The peakedness of the distribution.

### Exercise 4: Visualizing Data with psych

```{r}
# Sample data
multi_var_data <- data.frame(
  score = c(85, 90, 78, 92, 88, 76, 95, 89, 84, 91),
  age = c(23, 25, 22, 24, 26, 21, 27, 25, 23, 24),
  study_hours = c(5, 6, 4, 6, 5, 3, 7, 6, 5, 6)
)

# Create the correlation plot
corMatrix <- cor(multi_var_data)
corPlot(corMatrix, numbers = TRUE, main = "Correlation Matrix")
```

**Interpretation**:

-   The correlation coefficients indicate the strength and direction of the relationships between variables.

-   Positive correlations: Variables increase together.

-   Negative correlations: One variable increases while the other decreases.

-   The numbers and colors help visualize these relationships.

```{r}
# Create the pair panels
pairs.panels(multi_var_data, 
             method = "pearson",  # correlation method
             hist.col = "blue",    # histogram color
             density = TRUE,       # add density plots
             ellipses = TRUE       # add correlation ellipses
)

```

**Interpretation**:

-   Scatterplots in the lower triangle show relationships between pairs of variables.

-   Histograms on the diagonal show the distribution of each variable.

-   Correlation coefficients in the upper triangle indicate the strength and direction of relationships.

-   Density plots add information about data concentration.

-   Correlation ellipses provide a visual representation of confidence intervals for the correlations.

## Answers to Chapter 6 Practice Exercises

### Exercise 1: Mean-Centering

**Dataset**:
- `expenses <- c(1200, 1500, 1100, 1800, 1300, 1700, 1250, 1400, 1600, 1350)`

**Tasks and Answers**:

1. **Calculate the mean of the expenses**:
```{r}
expenses <- c(1200, 1500, 1100, 1800, 1300, 1700, 1250, 1400, 1600, 1350)

mean_expenses <- mean(expenses)
mean_expenses
# Answer: 1425
```

2. **Mean-center the dataset by subtracting the mean from each value**:
```{r}
   mean_centered_expenses <- expenses - mean_expenses
   mean_centered_expenses
   # Answer: -225, 75, -325, 375, -125, 275, -175, -25, 175, -75
```

3. **Plot the original and mean-centered expenses on the same graph**:
```{r}
   y_limits <- range(c(expenses, mean_centered_expenses))
   plot(expenses, type = "b", col = "blue", ylab = "Expenses", xlab = "Month", main = "Original vs Mean-Centered Expenses", ylim = y_limits)
   lines(mean_centered_expenses, type = "b", col = "red")
   legend("topright", legend = c("Original", "Mean-Centered"), col = c("blue", "red"), lty = 1)
```

**Interpretation**:  
   - **Answer**: A positive mean-centered value indicates that the expense for that month is above the average expense, while a negative value indicates that the expense is below the average. Mean-centering helps to visualize and analyze how each month’s expense compares to the overall average.  

### Exercise 2: Z-Scores

**Dataset**:
- `test_scores <- c(65, 78, 82, 91, 70, 88, 75, 95, 80, 85)`

**Tasks and Answers**:

1. **Calculate the mean and standard deviation of the test scores**:
```{r}
test_scores <- c(65, 78, 82, 91, 70, 88, 75, 95, 80, 85)
   mean_test_scores <- mean(test_scores)
   sd_test_scores <- sd(test_scores)
   mean_test_scores
   sd_test_scores
   # Answer: Mean = 79.9, Standard Deviation = 9.9
```

2. **Compute the Z-scores for each test score**:
```{r}
   z_scores <- (test_scores - mean_test_scores) / sd_test_scores
   z_scores
   # Answer: -1.50, -0.19, 0.21, 1.11, -0.99, 0.82, -0.50, 1.53, 0.01, 0.51
```

3. **Create a histogram of the Z-scores and add a vertical line at Z = 0**:
```{r}
   hist(z_scores, breaks = 10, col = "blue", xlab = "Z-Scores", main = "Histogram of Z-Scores")
   abline(v = 0, col = "red", lwd = 2)
```

**Interpretation**:  
   - **Answer**: A Z-score greater than 0 indicates that the test score is above the average, while a Z-score less than 0 indicates that the test score is below the average. Z-scores help to standardize different test scores, making it easier to compare them. Outliers are typically identified as Z-scores beyond ±2 or ±3.

### Exercise 3: Combining Mean-Centering and Z-Scores

**Dataset**:
- `reaction_times <- c(250, 340, 295, 310, 275, 325, 290, 360, 285, 310)`

**Tasks and Answers**:

1. **Mean-center the reaction times**:
```{r}
reaction_times <- c(250, 340, 295, 310, 275, 325, 290, 360, 285, 310)
   mean_reaction_time <- mean(reaction_times)
   mean_centered_times <- reaction_times - mean_reaction_time
   mean_centered_times
   # Answer: -51, 39, -6, 9, -26, 24, -11, 59, -16, 9
```

2. **Calculate the Z-scores for the mean-centered reaction times**:
```{r}
   sd_reaction_time <- sd(reaction_times)
   z_scores_centered <- mean_centered_times / sd_reaction_time
   z_scores_centered
   # Answer: -1.42, 1.08, -0.17, 0.25, -0.73, 0.68, -0.31, 1.68, -0.45, 0.25
```

3. **Plot the original reaction times, mean-centered times, and Z-scores on separate graphs**:
```{r}
   par(mfrow = c(3, 1))
   plot(reaction_times, type = "b", col = "blue", ylab = "Reaction Times", xlab = "Index", main = "Original Reaction Times")
   plot(mean_centered_times, type = "b", col = "green", ylab = "Mean-Centered", xlab = "Index", main = "Mean-Centered Reaction Times")
   plot(z_scores_centered, type = "b", col = "red", ylab = "Z-Scores", xlab = "Index", main = "Z-Scores of Mean-Centered Reaction Times")
```

**Interpretation**:  
   - **Answer**: Mean-centering adjusts the reaction times by subtracting the average, making it easier to see how each participant's time compares to the average. Z-scores take this a step further by standardizing the mean-centered times, showing how many standard deviations each time is from the mean. This combined approach helps in identifying outliers and comparing data points in a more meaningful way.

### Exercise 4: Non-Linear Transformations

**Dataset**:
- `income <- c(30, 45, 70, 120, 25, 60, 100, 85, 40, 300)`

**Tasks and Answers**:

1. **Apply a logarithmic transformation to the income data**:
```{r}
   income <- c(30, 45, 70, 120, 25, 60, 100, 85, 40, 300)
   log_income <- log(income)
   log_income
   # Answer: 3.40, 3.81, 4.25, 4.79, 3.22, 4.09, 4.61, 4.44, 3.69, 5.70
```

2. **Apply a square root transformation to the income data**:
```{r}
   sqrt_income <- sqrt(income)
   sqrt_income
   # Answer: 5.48, 6.71, 8.37, 10.95, 5.00, 7.75, 10.00, 9.22, 6.32, 17.32
```

3. **Apply an inverse transformation to the income data**:
```{r}
   inv_income <- 1 / income
   inv_income
   # Answer: 0.0333, 0.0222, 0.0143, 0.0083, 
```

4. **Plot histograms of the original and transformed datasets**:
```{r}
   par(mfrow = c(2, 2))
   hist(income, breaks = 10, col = "blue", xlab = "Income", main = "Original Income")
   hist(log_income, breaks = 10, col = "green", xlab = "Log(Income)", main = "Log Transformed Income")
   hist(sqrt_income, breaks = 10, col = "orange", xlab = "Sqrt(Income)", main = "Square Root Transformed Income")
   hist(inv_income, breaks = 10, col = "red", xlab = "1/Income", main = "Inverse Transformed Income")
```

**Interpretation**:  
   - **Answer**:   
     - **Logarithmic Transformation**: Reduces skewness by pulling in large values, making the distribution more balanced. Useful when dealing with right-skewed data, such as income.  
     - **Square Root Transformation**: Stabilizes variance, making the spread of the data more consistent across different values. Useful for data where variability increases with the value.  
     - **Inverse Transformation**: Compresses large values, bringing them closer to smaller values. Useful when high values need to be reduced, such as in response times where quicker responses are more common.

## Answers to Chapter 7 Practice Exercises

### Exercise 1: Create an APA-Compliant Bar Graph

**Objective**: Create a bar graph comparing the mean values of a categorical variable, including error bars to represent variability.

**Solution**:

```{r, warning = F, message = F}
library(ggplot2)

# Create the APA-compliant bar graph
ggplot(mtcars, aes(x = factor(am), y = mpg)) +
  geom_bar(stat = "summary", fun = "mean", fill = "lightblue", color = "black") +
  geom_errorbar(stat = "summary", fun.data = "mean_se", width = 0.2, color = "black") +
  labs(title = "Average MPG by Transmission Type",
       x = "Transmission (0 = Automatic, 1 = Manual)",
       y = "Miles Per Gallon") +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(color = "black", size = 0.5, fill = NA)
  )
```

**Explanation**:  
- The `factor(am)` converts the transmission variable into a factor for categorical comparison.  
- `geom_bar()` creates the bar graph, while `geom_errorbar()` adds error bars representing the standard error of the mean.  
- APA formatting is applied using `theme_minimal()` with additional customization to meet APA standards.

### Exercise 2: Modify a Basic ggplot2 Plot to Meet APA Standards

**Objective**: Modify a basic scatter plot to adhere to APA formatting guidelines.

**Solution**:

```{r}
# Create the basic scatter plot
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point(size = 3) +
  labs(title = "Scatter Plot of Weight and MPG",
       x = "Weight (1000 lbs)",
       y = "Miles Per Gallon") +
  geom_smooth(method = "lm", se = FALSE, color = "black", linetype = "dashed", size = 0.7) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(color = "black", size = 0.5, fill = NA)
  )
```

**Explanation**:  
- A basic scatter plot is created with `geom_point()`.  
- A trend line is added using `geom_smooth(method = "lm", se = FALSE)`.  
- The plot is customized to meet APA standards by adjusting font sizes, adding a dashed trend line, and removing unnecessary grid lines.

### Exercise 3: Create an APA-Compliant Line Graph and Save It as a High-Resolution Image

**Objective**: Create a line graph comparing trends across groups, and save the graph as a high-resolution image.

**Solution**:

```{r}
# Create the APA-compliant line graph
p <- ggplot(mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +
  geom_line(size = 1, linetype = "solid") +
  labs(title = "MPG vs. Weight by Cylinder Count",
       x = "Weight (1000 lbs)",
       y = "Miles Per Gallon",
       color = "Cylinders") +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(color = "black", size = 0.5, fill = NA),
    legend.position = "right"
  )

# Save the graph as a high-resolution PNG file
ggsave("mpg_vs_weight_by_cyl.png", plot = p, width = 8, height = 6, dpi = 300)
```

**Explanation**:  
- The line graph is created using `geom_line()`, with different colors representing different cylinder counts.  
- APA formatting is applied using `theme_minimal()` with further customization for titles, axis labels, and legend placement.  
- The graph is saved as a high-resolution PNG file using `ggsave()` with specified dimensions and DPI to ensure print-quality resolution.

## Answers to Chapter 8 Practice Exercises

### Exercise 1: Create a Simple Bivariate Linear Model

**Objective**: Create a bivariate linear model using the provided dataset, and interpret the slope, intercept, and residuals.  

**Solution**:  

```{r}
# Simulating the provided data
hours_studied <- c(2, 3, 5, 6, 8, 10)
exam_scores <- c(68, 72, 78, 85, 90, 95)

# Creating the linear model
model <- lm(exam_scores ~ hours_studied)

# Viewing the summary of the model
summary(model)

# Calculating the residuals
residuals <- residuals(model)

# Displaying residuals
residuals
```

**Interpretation**:  

- **Slope**: Suppose the model output shows that the slope (`b1`) is 3.5. This means that for every additional hour studied, the exam score is expected to increase by 3.5 points. This indicates a positive relationship between study time and exam performance.  
  
- **Intercept**: Let’s say the intercept (`b0`) is 65. This suggests that if a student does not study at all (0 hours studied), their predicted exam score would be 65. The intercept provides a baseline score, representing the score a student might achieve without any study time.  

- **Residuals**: The residuals represent the differences between the observed exam scores and those predicted by the model. For example, if a student who studied for 6 hours scored 85, but the model predicted a score of 83, the residual would be 2 (85 - 83). If the residuals are small, it indicates that the model’s predictions are close to the actual data. In this case, the residuals might be small, suggesting that the model fits the data well.  

### Exercise 2: Analyze Residuals to Assess Model Fit

**Objective**: Analyze the residuals of a linear model to assess its fit and discuss any patterns you observe.  

**Solution**:  

```{r}
# Plotting the residuals
plot(hours_studied, residuals, main = "Residuals Plot",
     xlab = "Hours Studied", ylab = "Residuals", pch = 19, col = "blue")
abline(h = 0, col = "red", lwd = 2)
```

**Interpretation**:  

- **Random Distribution**: If the residuals are randomly scattered around the horizontal line at zero, this suggests that the model fits the data well, with no systematic errors. In this exercise, the residuals might appear to be randomly distributed, indicating that the linear model is appropriate for this dataset.  

- **Patterns**: If the residuals showed a pattern (e.g., they systematically increase or decrease), it might indicate that the model is not capturing some aspect of the data. For example, if residuals consistently increase as study hours increase, this could suggest a nonlinear relationship that a simple linear model cannot capture.  

**Conclusion**: Assuming the residuals are randomly distributed in this scenario, you can conclude that the linear model is a good fit for the data. There are no evident patterns in the residuals, suggesting that the model appropriately captures the relationship between study hours and exam scores.

### Exercise 3: Apply Bivariate Linear Models to a Real-World Dataset

**Objective**: Apply what you’ve learned to analyze a real-world psychological dataset and interpret the results of your linear model.  

**Solution**:  

```{r}
# Simulating the dataset
set.seed(123)
anxiety <- rnorm(100, mean = 50, sd = 15)  # Anxiety scores (0 to 100)
sleep_hours <- 8 - 0.04 * anxiety + rnorm(100, mean = 0, sd = 1)  # Sleep hours

# Combining into a data frame
data <- data.frame(anxiety, sleep_hours)

# Creating the linear model
model <- lm(sleep_hours ~ anxiety, data = data)

# Viewing the summary of the model
summary(model)

# Plotting the residuals
residuals <- residuals(model)
plot(data$anxiety, residuals, main = "Residuals Plot",
     xlab = "Anxiety Levels", ylab = "Residuals", pch = 19, col = "blue")
abline(h = 0, col = "red", lwd = 2)
```

**Interpretation**:  

- **Slope**: Suppose the slope (`b1`) is -0.04. This suggests that for each additional point increase in anxiety score, the number of hours of sleep decreases by 0.04 hours (or approximately 2.4 minutes). This indicates a negative relationship between anxiety levels and sleep hours, meaning higher anxiety is associated with less sleep.  

- **Intercept**: If the intercept (`b0`) is 8, it represents the predicted number of sleep hours when anxiety is zero. This suggests that in the absence of anxiety, the expected sleep time is 8 hours.  

- **P-Value**: Let’s assume the p-value associated with the slope is 0.02. Since this value is less than 0.05, it indicates that the relationship between anxiety levels and sleep hours is statistically significant. This means there is a meaningful association between higher anxiety and reduced sleep, not due to random chance.  

- **Residuals**: If the residuals are randomly distributed around the horizontal line at zero in the residuals plot, this suggests that the linear model is appropriate for this data. If you notice any patterns (e.g., a systematic curve), it might indicate that the model is not capturing the relationship correctly, and you might need to consider a more complex model.  

**Conclusion**: The negative slope indicates that higher anxiety levels are associated with fewer hours of sleep. The statistically significant p-value supports this relationship, suggesting it is unlikely to have occurred by chance. The residuals plot confirms that the linear model is a good fit for the data, as the residuals appear randomly distributed with no apparent pattern. This analysis provides evidence that managing anxiety could be crucial for improving sleep quality.  


------------------------------------------------------------------------

This appendix will be continuously updated as new exercises and chapters are added to the textbook, providing a comprehensive resource for students to check their work and ensure they understand the material thoroughly.
:::

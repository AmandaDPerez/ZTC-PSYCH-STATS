# Multiple Regression

## Introduction to Multiple Regression

### What is Multiple Regression?

Multiple regression is a powerful statistical method that allows us to explore and understand the relationships between one dependent variable (often referred to as the outcome variable) and two or more independent variables (also called predictors). Unlike bivariate regression, which looks at the relationship between two variables, multiple regression lets us consider several predictors at once.

Think of multiple regression as a way to understand how different factors work together to influence an outcome. For example, if we want to predict a student's academic performance (our dependent variable), we might look at several factors, such as the number of hours they study, the quality of their sleep, and their stress levels. Each of these factors is an independent variable that might contribute to how well the student performs academically.

Multiple regression extends the idea of bivariate regression by allowing us to include more than one predictor in our analysis. This is especially useful when we believe that several factors are influencing an outcome, and we want to understand the unique contribution of each one.

In simple terms, while bivariate regression might tell us that "more study hours lead to better grades," multiple regression can tell us "how much study hours, sleep quality, and stress levels together contribute to better grades."

### Why Use Multiple Regression?

Multiple regression is incredibly valuable in psychological research because it allows us to untangle complex relationships between variables. Here's why it's so useful:

1.  **Control for Confounding Variables**: In real life, many factors often influence an outcome. If we only look at one factor at a time (as in bivariate regression), we might miss how other factors are playing a role. Multiple regression helps us control for these confounding variables, ensuring that we can see the true relationship between each predictor and the outcome.

    -   **Example**: Imagine we're studying the effect of sleep on academic performance. If we only consider sleep quality, we might miss how study habits also play a role. By including both sleep and study habits in a multiple regression, we can see how each one independently affects academic performance.

2.  **Examine the Unique Contribution of Each Predictor**: Multiple regression allows us to see how each independent variable uniquely contributes to the outcome, even when other variables are in the mix. This helps us understand the specific role each factor plays.

    -   **Example**: In predicting academic performance, we might find that both study habits and sleep quality are important, but stress levels also play a significant role. Multiple regression can show us how much of an impact each of these factors has, separately from the others.

3.  **More Accurate Predictions**: Because multiple regression takes multiple factors into account, it can often make more accurate predictions about outcomes than bivariate regression, which looks at only one factor at a time.

    -   **Example**: A model that predicts academic performance based on study habits, sleep quality, and stress levels is likely to be more accurate than a model that only considers study habits.

In psychological research, multiple regression is particularly useful when we want to understand how various aspects of a person's life interact to influence behaviors, outcomes, or conditions. Whether we're predicting mental health outcomes based on a combination of factors like social support, exercise, and stress, or understanding how different parenting styles contribute to a child's development, multiple regression provides a deeper and more comprehensive analysis than simpler methods.

## Understanding Main Effects in Multiple Regression

### The Concept of Main Effects

When we talk about **main effects** in multiple regression, we're referring to the unique contribution that each independent variable (or predictor) makes to the dependent variable (or outcome) when we consider all the predictors together.

Think of each independent variable as a separate piece of the puzzle that helps us predict the outcome. In multiple regression, we're interested in understanding how much each piece (or predictor) contributes to the whole picture (the outcome), independently of the other pieces.

For example, imagine you're trying to predict a student's exam score based on three factors: how many hours they study, the quality of their sleep, and their stress levels. Each of these factors is an independent variable, and the exam score is the dependent variable. The **main effect** of each factor is how much it contributes to the exam score when we consider the effects of the other factors at the same time.

It's important to focus on main effects when we want to understand the direct relationship between each predictor and the outcome. By looking at main effects, we can see how much of the outcome can be explained by each predictor, without being confused by the influence of the other predictors.

### Interpreting Main Effects in Multiple Regression

In multiple regression, each independent variable gets a **coefficient** (sometimes called a slope), which tells us about the main effect of that variable on the dependent variable. This coefficient answers the question: "How much does the outcome change when this predictor changes, while keeping all the other predictors the same?"

**How to Interpret the Coefficients:**

-   **Positive Coefficient**: If the coefficient is positive, it means that as the predictor increases, the outcome also increases.
-   **Negative Coefficient**: If the coefficient is negative, it means that as the predictor increases, the outcome decreases.

Let's look at an example to make this clearer.

**Example: Predicting Exam Scores**

Suppose you've run a multiple regression analysis to predict exam scores based on study time, sleep quality, and stress levels. Here's what the results might look like:

-   **Study Time Coefficient**: 3
    -   Interpretation: For every additional hour of study time, the exam score increases by 3 points, **holding sleep quality and stress levels constant**.
-   **Sleep Quality Coefficient**: 2
    -   Interpretation: For every one-unit increase in sleep quality (on a scale from 1 to 10), the exam score increases by 2 points, **holding study time and stress levels constant**.
-   **Stress Levels Coefficient**: -1.5
    -   Interpretation: For every one-unit increase in stress levels (on a scale from 1 to 10), the exam score decreases by 1.5 points, **holding study time and sleep quality constant**.

**Language for Interpretation:**

When interpreting these coefficients, we often use language like this:

***"For every one-unit increase in [predictor variable], [dependent variable] increases/decreases by [coefficient value], holding all other variables constant."***

For example:

-   "For every additional hour of study time, exam scores increase by 3 points, holding sleep quality and stress levels constant."
-   "For every one-unit increase in stress levels, exam scores decrease by 1.5 points, holding study time and sleep quality constant."

This way of interpreting the coefficients helps us understand the unique contribution of each predictor to the outcome. It tells us how much of the change in the outcome can be attributed to each predictor when we account for the influence of the other predictors.

Understanding main effects in multiple regression is crucial because it allows us to see the specific role that each predictor plays in determining the outcome. This insight is especially important in psychological research, where multiple factors often interact to influence behavior, performance, and other outcomes.

## Calculating and Interpreting Multiple Regression in R

Now that we've introduced the concept of multiple regression and discussed how to interpret the main effects, it's time to see how this works in practice using R. In this section, we'll walk through the process of running a multiple regression analysis in R, step by step, and then interpret the output to understand what the results mean.

### Step-by-Step Guide to Running a Multiple Regression in R

Running a multiple regression in R is straightforward, and the process is similar to running a bivariate regression, but with more than one predictor variable. Let's break it down.

**Example Scenario**: Imagine you're studying the factors that predict anxiety levels in individuals. You believe that anxiety levels are influenced by sleep quality, exercise frequency, and social support. Here's how you would use R to run a multiple regression analysis with these predictors.

**Step 1: Prepare Your Data** Before running the regression, you need to have your data ready. Let's assume you have the following variables:

-   **Anxiety**: The dependent variable (e.g., anxiety levels on a scale of 1-100).
-   **Sleep_Quality**: An independent variable (e.g., sleep quality on a scale of 1-10).
-   **Exercise_Frequency**: Another independent variable (e.g., number of exercise sessions per week).
-   **Social_Support**: Another independent variable (e.g., social support level on a scale of 1-10).

Here's a simple dataset:

``` r
# Sample data
Anxiety <- c(55, 65, 70, 45, 50, 60, 75, 80, 67, 59)
Sleep_Quality <- c(8, 7, 6, 9, 8, 5, 4, 3, 6, 7)
Exercise_Frequency <- c(3, 4, 2, 5, 3, 1, 0, 1, 2, 4)
Social_Support <- c(9, 8, 7, 8, 9, 5, 4, 3, 6, 7)
```

**Step 2: Run the Multiple Regression** To run the multiple regression, use the `lm()` function in R, which stands for "linear model."

``` r
# Run multiple regression
model <- lm(Anxiety ~ Sleep_Quality + Exercise_Frequency + Social_Support)
summary(model)
```

In this code:

\- `Anxiety ~ Sleep_Quality + Exercise_Frequency + Social_Support` specifies that we are predicting Anxiety levels based on the predictors Sleep_Quality, Exercise_Frequency, and Social_Support.

\- `summary(model)` provides a detailed summary of the regression analysis, including the coefficients, p-values, and R-squared values.

### Interpreting the Output of a Multiple Regression Model

Once you run the regression, R will produce an output that includes several key components. Let's go through what each part means and how to interpret it.

**Key Components of the Output:**

1.  **Coefficients**: These are the slopes (or main effects) for each predictor variable. They tell you how much the dependent variable (Anxiety) changes for each one-unit change in the predictor, holding all other variables constant.

    -   **Example Coefficient Output**:

        ```         
        Coefficients:
                         Estimate Std. Error t value Pr(>|t|)   
        (Intercept)          45.00      6.00   7.50   <2e-16 ***
        Sleep_Quality       -2.50      0.80  -3.13   0.0056 **
        Exercise_Frequency  -1.00      0.90  -1.11   0.2987    
        Social_Support      -2.00      0.70  -2.86   0.0089 **
        ```

    -   **Interpreting the Coefficients**:

        -   **Intercept**: The intercept is the predicted value of Anxiety when all the predictors are zero. In this case, it's 45.
        -   **Sleep_Quality**: For every one-unit increase in Sleep Quality, Anxiety decreases by 2.5 points, holding Exercise Frequency and Social Support constant.
        -   **Exercise_Frequency**: For every one-unit increase in Exercise Frequency, Anxiety decreases by 1 point, though this result is not statistically significant (p-value \> 0.05).
        -   **Social_Support**: For every one-unit increase in Social Support, Anxiety decreases by 2 points, holding Sleep Quality and Exercise Frequency constant.

2.  **P-Values**: These values tell you whether the relationship between each predictor and the dependent variable is statistically significant.

    -   **Significance Levels**:
        -   `***`: Highly significant (p \< 0.001)
        -   `**`: Significant (p \< 0.01)
        -   `*`: Marginally significant (p \< 0.05)
    -   In our example, Sleep Quality and Social Support have significant p-values, suggesting they are important predictors of Anxiety. Exercise Frequency, however, does not have a significant p-value, indicating it may not be a strong predictor in this model.

3.  **R-Squared Value**: This value tells you how much of the variance in the dependent variable (Anxiety) is explained by the model. It ranges from 0 to 1, with higher values indicating a better fit.

    -   **Example R-Squared Output**:

        ```         
        Multiple R-squared:  0.75, Adjusted R-squared:  0.68 
        ```

    -   **Interpreting R-Squared**:

        -   In this example, 75% of the variance in Anxiety levels is explained by the combination of Sleep Quality, Exercise Frequency, and Social Support. This suggests that these predictors, together, provide a good explanation of the variability in Anxiety levels.

**Discussion on Practical Significance**:

While the statistical significance (p-values) tells you whether the predictors have a significant relationship with the outcome, the practical significance is about how much of an impact these predictors have in real-world terms.

-   **Sleep Quality**: A coefficient of -2.5 suggests that improving sleep quality by one unit could lead to a noticeable decrease in anxiety levels. If this effect is significant (as the p-value suggests), it could be practically important for interventions aimed at reducing anxiety.
-   **Social Support**: Similarly, increasing social support by one unit might reduce anxiety by 2 points, which could be practically significant, especially in a therapeutic or counseling context.
-   **Exercise Frequency**: Although exercise frequency has a negative coefficient, suggesting that more exercise might reduce anxiety, the lack of statistical significance suggests that, in this model, it might not be a key factor in predicting anxiety levels.

By understanding these components, you can interpret the output of a multiple regression model in a meaningful way, allowing you to draw conclusions about the relationships between your predictors and the outcome variable. This process is crucial for making informed decisions in psychological research, where multiple factors often interact to influence behaviors, emotions, and outcomes.

## Understanding Suppression in Multiple Regression

Multiple regression is a powerful tool because it allows us to see how several predictors work together to influence an outcome. However, sometimes the relationships between variables aren't as straightforward as they seem, and this is where the concept of suppression comes in. Suppression can help reveal hidden relationships that aren't apparent when we look at variables in isolation.

### What is Suppression?

**Suppression** occurs in multiple regression when adding an additional predictor to the model actually increases the predictive power of another predictor. This might seem counterintuitive at first---why would including a new variable make another one more predictive? But this happens because the new predictor controls for or accounts for certain aspects of the data, allowing the true relationship of another variable to shine through.

**Example**: Imagine you're studying the relationship between sleep quality and academic performance. You might expect that better sleep quality leads to better academic performance. However, when you run a bivariate analysis, you find that the relationship is weak or even non-existent.

Now, suppose you add stress levels as a predictor in a multiple regression model. Suddenly, the relationship between sleep quality and academic performance becomes much stronger. What's happening here is **suppression**: stress levels were masking the true relationship between sleep and performance. By including stress levels in the model, you're able to see the real impact of sleep quality.

Suppression reveals hidden relationships that wouldn't be visible if we only looked at variables one at a time. It shows us how complex and intertwined the factors influencing an outcome can be.

### Identifying Suppression Effects

Identifying suppression effects in a multiple regression model involves comparing the results of a bivariate analysis with those of a multiple regression analysis.

**Step-by-Step Guide**:

1.  **Run a Bivariate Regression**: Start by running a simple regression analysis with just one predictor (e.g., sleep quality) and the outcome variable (e.g., academic performance).

    ``` r
    # Bivariate regression
    model_bivariate <- lm(Academic_Performance ~ Sleep_Quality)
    summary(model_bivariate)
    ```

2.  **Add a Potential Suppressor Variable**: Next, add a potential suppressor variable to the model (e.g., stress levels) and run the multiple regression.

    ``` r
    # Multiple regression with suppressor
    model_multiple <- lm(Academic_Performance ~ Sleep_Quality + Stress_Levels)
    summary(model_multiple)
    ```

3.  **Compare the Coefficients**: Compare the coefficients for sleep quality in the bivariate model and the multiple regression model. If the coefficient for sleep quality becomes stronger (more predictive) in the multiple regression, this suggests that stress levels were suppressing the relationship between sleep quality and academic performance.

**Example**: Let's say you find the following results:

-   **Bivariate Model (Sleep Quality Only)**: Coefficient for Sleep Quality = 0.5
-   **Multiple Regression (Sleep Quality + Stress Levels)**: Coefficient for Sleep Quality = 1.5

In this case, the inclusion of stress levels increased the coefficient for sleep quality from 0.5 to 1.5, indicating that stress was suppressing the true impact of sleep quality on academic performance.

**Visual Representation**:

You can visualize suppression effects using R and ggplot2 by plotting the relationships between the predictors and the outcome variable. For example:

```{r}
library(ggplot2)

# Sample data
Sleep_Quality <- c(8, 7, 6, 9, 8, 5, 4, 3, 6, 7)
Academic_Performance <- c(75, 70, 68, 80, 77, 60, 58, 55, 65, 70)
Stress_Levels <- c(5, 6, 7, 4, 5, 8, 9, 10, 7, 6)

# Visualize the relationship between Sleep Quality and Academic Performance
ggplot(data = data.frame(Sleep_Quality, Academic_Performance), aes(x = Sleep_Quality, y = Academic_Performance)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  labs(title = "Relationship between Sleep Quality and Academic Performance", x = "Sleep Quality", y = "Academic Performance")

# Visualize the relationship between Sleep Quality and Academic Performance, controlling for Stress Levels
# Use partial residuals or similar techniques
```

This kind of plot can help you see how the relationship changes when you control for the suppressor variable (in this case, stress levels).

### Practical Implications of Suppression

Understanding suppression effects has important implications for psychological research:

1.  **Revealing True Relationships**: Suppression can uncover the true relationships between variables that might be hidden due to the influence of other factors. This leads to a more accurate understanding of how different predictors influence an outcome.

2.  **Improving Model Accuracy**: By identifying and accounting for suppressor variables, you can create more accurate and predictive models. This is particularly important in fields like psychology, where many variables often interact in complex ways.

3.  **Enhancing Interpretation**: Recognizing suppression effects allows researchers to make more informed interpretations of their data. It highlights the importance of considering all relevant variables when analyzing relationships and prevents misleading conclusions that might arise from bivariate analyses alone.

**Example**: Suppose a researcher finds that adding a variable for social support increases the predictive power of a model examining the relationship between exercise frequency and depression. This suggests that social support was a suppressor, and by including it, the researcher can now see the true impact of exercise on depression. Recognizing this effect might lead to new insights and potentially more effective interventions.

In summary, suppression is a valuable concept in multiple regression that helps researchers uncover hidden relationships and improve the accuracy of their models. By understanding and identifying suppression effects, you can gain deeper insights into the complex interactions between variables in psychological research.
